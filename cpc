#!/bin/bash

# Color definitions
export GREEN='\033[32m'
export RED='\033[0;31m'
export YELLOW='\033[0;33m'
export BLUE='\033[1;34m'
export ENDCOLOR='\033[0m'

# Configuration
CONFIG_DIR="$HOME/.config/my-kthw-cpc" # Changed from my-kthw-ccr
REPO_PATH_FILE="$CONFIG_DIR/repo_path"
CLUSTER_CONTEXT_FILE="$CONFIG_DIR/current_cluster_context"
CPC_ENV_FILE="cpc.env" # Expect this in the repo root, Changed from CCR_ENV_FILE and ccr.env

# --- Helper Functions ---
check_required_commands() {
  for cmd in "$@"; do
    if ! command -v "$cmd" &> /dev/null; then
      echo -e "${RED}Error: '$cmd' is required but not installed. Please install it before proceeding.${ENDCOLOR}" >&2
      exit 1
    fi
  done
}
export -f check_required_commands

get_repo_path() {
  if [ -f "$REPO_PATH_FILE" ]; then
    cat "$REPO_PATH_FILE"
  else
    echo -e "${RED}Repository path not set. Run 'cpc setup-cpc' to set this value.${ENDCOLOR}" >&2 # Changed from ccr setup-ccr
    exit 1
  fi
}
export -f get_repo_path

get_current_cluster_context() {
  if [ -f "$CLUSTER_CONTEXT_FILE" ]; then
    cat "$CLUSTER_CONTEXT_FILE"
  else
    echo -e "${RED}Error: No cpc context set.${ENDCOLOR}" >&2
    echo -e "${BLUE}The cpc context determines the Tofu workspace and associated configuration (e.g., OS type).${ENDCOLOR}" >&2
    echo -e "${BLUE}Please set a context using 'cpc ctx <workspace_name>'.${ENDCOLOR}" >&2
    
    # Attempt to get repo_path to list workspaces.
    # This relies on REPO_PATH_FILE being set by 'cpc setup-cpc'.
    if [ -f "$REPO_PATH_FILE" ]; then
      local repo_p_for_listing
      repo_p_for_listing=$(cat "$REPO_PATH_FILE")
      if [ -d "$repo_p_for_listing/terraform" ]; then
        echo -e "${BLUE}Available Tofu workspaces in '$repo_p_for_listing/terraform' (use one of these for <workspace_name>):${ENDCOLOR}" >&2
        # Ensure tofu command is available for listing or provide a message
        if command -v tofu &> /dev/null; then
          (cd "$repo_p_for_listing/terraform" && tofu workspace list | sed 's/^*/  /') >&2
        else
          echo -e "${YELLOW}  'tofu' command not found. Cannot list workspaces. Please ensure OpenTofu is installed and in your PATH.${ENDCOLOR}" >&2
        fi
      else
        echo -e "${YELLOW}Warning: Cannot list Tofu workspaces. Terraform directory not found at '$repo_p_for_listing/terraform'.${ENDCOLOR}" >&2
      fi
    else
      echo -e "${YELLOW}Warning: Cannot list Tofu workspaces. Repository path not set. Run 'cpc setup-cpc'.${ENDCOLOR}" >&2
    fi
    echo -e "${BLUE}Typically, the context/workspace should be one of: debian, ubuntu, rocky.${ENDCOLOR}" >&2
    exit 1
  fi
}
export -f get_current_cluster_context

# Check if secrets are already loaded
check_secrets_loaded() {
  if [ -z "$PROXMOX_HOST" ] || [ -z "$PROXMOX_USERNAME" ] || [ -z "$VM_USERNAME" ]; then
    echo -e "${RED}Error: Secrets not loaded. This command requires SOPS secrets to be loaded.${ENDCOLOR}" >&2
    echo -e "${BLUE}Please run 'cpc load_secrets' first or ensure cpc.env is properly configured.${ENDCOLOR}" >&2
    exit 1
  fi
}
export -f check_secrets_loaded

# Load sensitive data from secrets.sops.yaml using SOPS
load_secrets() {
  local repo_root
  repo_root=$(get_repo_path)
  local secrets_file="$repo_root/terraform/secrets.sops.yaml"
  
  if [ ! -f "$secrets_file" ]; then
    echo -e "${RED}Error: secrets.sops.yaml not found at $secrets_file${ENDCOLOR}" >&2
    exit 1
  fi
  
  # Check if sops is installed
  if ! command -v sops &> /dev/null; then
    echo -e "${RED}Error: 'sops' is required but not installed. Please install it before proceeding.${ENDCOLOR}" >&2
    exit 1
  fi
  
  # Check if jq is installed
  if ! command -v jq &> /dev/null; then
    echo -e "${RED}Error: 'jq' is required but not installed. Please install it before proceeding.${ENDCOLOR}" >&2
    exit 1
  fi
  
  echo -e "${BLUE}Loading secrets from secrets.sops.yaml...${ENDCOLOR}"
  
  # Export sensitive variables from SOPS
  export PROXMOX_HOST
  export PROXMOX_USERNAME  
  export PROXMOX_PASSWORD
  export VM_USERNAME
  export VM_PASSWORD
  export VM_SSH_KEY
  
  # Load secrets using sops, convert to JSON, then parse with jq
  local secrets_json
  secrets_json=$(sops -d "$secrets_file" 2>/dev/null | python3 -c "import sys, yaml, json; json.dump(yaml.safe_load(sys.stdin), sys.stdout)")
  
  if [ $? -ne 0 ]; then
    echo -e "${RED}Error: Failed to decrypt secrets.sops.yaml. Check your SOPS configuration.${ENDCOLOR}" >&2
    exit 1
  fi
  
  # Parse secrets from JSON
  PROXMOX_HOST=$(echo "$secrets_json" | jq -r '.virtual_environment_endpoint' | sed 's|https://||' | sed 's|:8006/api2/json||')
  PROXMOX_USERNAME=$(echo "$secrets_json" | jq -r '.proxmox_username')
  PROXMOX_PASSWORD=$(echo "$secrets_json" | jq -r '.virtual_environment_password')
  VM_USERNAME=$(echo "$secrets_json" | jq -r '.vm_username')
  VM_PASSWORD=$(echo "$secrets_json" | jq -r '.vm_password')
  VM_SSH_KEY=$(echo "$secrets_json" | jq -r '.vm_ssh_keys[0]')
  
  # Verify that all required secrets were loaded
  if [ -z "$PROXMOX_HOST" ] || [ -z "$PROXMOX_USERNAME" ] || [ -z "$PROXMOX_PASSWORD" ] || [ -z "$VM_USERNAME" ] || [ -z "$VM_PASSWORD" ] || [ -z "$VM_SSH_KEY" ]; then
    echo -e "${RED}Error: Failed to load one or more required secrets from secrets.sops.yaml${ENDCOLOR}" >&2
    echo -e "${BLUE}Required secrets: PROXMOX_HOST, PROXMOX_USERNAME, PROXMOX_PASSWORD, VM_USERNAME, VM_PASSWORD, VM_SSH_KEY${ENDCOLOR}" >&2
    exit 1
  fi
  
  echo -e "${BLUE}Successfully loaded secrets (PROXMOX_HOST: $PROXMOX_HOST, VM_USERNAME: $VM_USERNAME)${ENDCOLOR}"
}
export -f load_secrets

# Source environment variables if cpc.env exists and set workspace-specific variables
load_env_vars() {
  local repo_root
  repo_root=$(get_repo_path)
  
  # Load secrets first
  load_secrets
  
  if [ -f "$repo_root/$CPC_ENV_FILE" ]; then # Changed from CCR_ENV_FILE
    # shellcheck source=cpc.env
    set -a # Automatically export all variables
    source "$repo_root/$CPC_ENV_FILE" # Changed from CCR_ENV_FILE
    set +a # Stop automatically exporting
    echo -e "${BLUE}Loaded environment variables from $CPC_ENV_FILE${ENDCOLOR}" # Changed from CCR_ENV_FILE
    
    # Set workspace-specific template variables based on current context
    # This function should only be called after REPO_PATH is set
    if [ -f "$CLUSTER_CONTEXT_FILE" ]; then
      local current_workspace
      current_workspace=$(cat "$CLUSTER_CONTEXT_FILE")
      set_workspace_template_vars "$current_workspace"
    fi
  else
    echo -e "${YELLOW}Warning: $CPC_ENV_FILE not found in repository root. Some default versions might be used by playbooks.${ENDCOLOR}" # Changed from CCR_ENV_FILE
  fi
}
export -f load_env_vars

# Set workspace-specific template variables based on current workspace
set_workspace_template_vars() {
  local workspace="$1"
  
  if [ -z "$workspace" ]; then
    echo -e "${YELLOW}Warning: No workspace specified for setting template variables${ENDCOLOR}"
    return 1
  fi
  
  # Convert workspace to uppercase for variable name construction
  local workspace_upper
  workspace_upper=$(echo "$workspace" | tr '[:lower:]' '[:upper:]')
  
  # Set template variables based on workspace
  case "$workspace" in
    debian)
      # Template VM configuration
      export TEMPLATE_VM_ID="${TEMPLATE_VM_ID_DEBIAN:-902}"
      export TEMPLATE_VM_NAME="${TEMPLATE_VM_NAME_DEBIAN:-tpl-debian-12-k8s}"
      export IMAGE_NAME="${IMAGE_NAME_DEBIAN:-debian-12-genericcloud-amd64.qcow2}"
      export IMAGE_LINK="${IMAGE_LINK_DEBIAN:-https://cloud.debian.org/images/cloud/bookworm/latest/debian-12-genericcloud-amd64.qcow2}"
      # Kubernetes component versions for debian
      export KUBERNETES_SHORT_VERSION="${KUBERNETES_SHORT_VERSION_DEBIAN:-1.30}"
      export KUBERNETES_MEDIUM_VERSION="${KUBERNETES_MEDIUM_VERSION_DEBIAN:-v1.30}"
      export KUBERNETES_VERSION="${KUBERNETES_MEDIUM_VERSION_DEBIAN:-v1.30}"
      export KUBERNETES_LONG_VERSION="${KUBERNETES_LONG_VERSION_DEBIAN:-1.30.0}"
      export CNI_PLUGINS_VERSION="${CNI_PLUGINS_VERSION_DEBIAN:-v1.5.0}"
      export CALICO_VERSION="${CALICO_VERSION_DEBIAN:-v3.27.0}"
      export METALLB_VERSION="${METALLB_VERSION_DEBIAN:-v0.14.5}"
      export COREDNS_VERSION="${COREDNS_VERSION_DEBIAN:-v1.11.1}"
      export METRICS_SERVER_VERSION="${METRICS_SERVER_VERSION_DEBIAN:-v0.7.1}"
      export ETCD_VERSION="${ETCD_VERSION_DEBIAN:-v3.5.12}"
      export KUBELET_SERVING_CERT_APPROVER_VERSION="${KUBELET_SERVING_CERT_APPROVER_VERSION_DEBIAN:-v0.1.9}"
      export LOCAL_PATH_PROVISIONER_VERSION="${LOCAL_PATH_PROVISIONER_VERSION_DEBIAN:-v0.0.26}"
      export CERT_MANAGER_VERSION="${CERT_MANAGER_VERSION_DEBIAN:-v1.16.2}"
      export ARGOCD_VERSION="${ARGOCD_VERSION_DEBIAN:-v2.13.2}"
      export INGRESS_NGINX_VERSION="${INGRESS_NGINX_VERSION_DEBIAN:-v1.12.0}"
      ;;
    ubuntu)
      # Template VM configuration
      export TEMPLATE_VM_ID="${TEMPLATE_VM_ID_UBUNTU:-912}"
      export TEMPLATE_VM_NAME="${TEMPLATE_VM_NAME_UBUNTU:-tpl-ubuntu-2404-k8s}"
      export IMAGE_NAME="${IMAGE_NAME_UBUNTU:-ubuntu-24.04-server-cloudimg-amd64.img}"
      export IMAGE_LINK="${IMAGE_LINK_UBUNTU:-https://cloud-images.ubuntu.com/releases/noble/release/ubuntu-24.04-server-cloudimg-amd64.img}"
      # Kubernetes component versions for ubuntu
      export KUBERNETES_SHORT_VERSION="${KUBERNETES_SHORT_VERSION_UBUNTU:-1.31}"
      export KUBERNETES_MEDIUM_VERSION="${KUBERNETES_MEDIUM_VERSION_UBUNTU:-v1.31}"
      export KUBERNETES_VERSION="${KUBERNETES_MEDIUM_VERSION_UBUNTU:-v1.31}"
      export KUBERNETES_LONG_VERSION="${KUBERNETES_LONG_VERSION_UBUNTU:-1.31.0}"
      export CNI_PLUGINS_VERSION="${CNI_PLUGINS_VERSION_UBUNTU:-v1.5.0}"
      export CALICO_VERSION="${CALICO_VERSION_UBUNTU:-v3.28.0}"
      export METALLB_VERSION="${METALLB_VERSION_UBUNTU:-v0.14.8}"
      export COREDNS_VERSION="${COREDNS_VERSION_UBUNTU:-v1.11.3}"
      export METRICS_SERVER_VERSION="${METRICS_SERVER_VERSION_UBUNTU:-v0.7.2}"
      export ETCD_VERSION="${ETCD_VERSION_UBUNTU:-v3.5.15}"
      export KUBELET_SERVING_CERT_APPROVER_VERSION="${KUBELET_SERVING_CERT_APPROVER_VERSION_UBUNTU:-v0.1.9}"
      export LOCAL_PATH_PROVISIONER_VERSION="${LOCAL_PATH_PROVISIONER_VERSION_UBUNTU:-v0.0.28}"
      export CERT_MANAGER_VERSION="${CERT_MANAGER_VERSION_UBUNTU:-v1.16.2}"
      export ARGOCD_VERSION="${ARGOCD_VERSION_UBUNTU:-v2.13.2}"
      export INGRESS_NGINX_VERSION="${INGRESS_NGINX_VERSION_UBUNTU:-v1.12.0}"
      ;;
    rocky)
      # Template VM configuration
      export TEMPLATE_VM_ID="${TEMPLATE_VM_ID_ROCKY:-931}"
      export TEMPLATE_VM_NAME="${TEMPLATE_VM_NAME_ROCKY:-tpl-rocky-9-k8s}"
      export IMAGE_NAME="${IMAGE_NAME_ROCKY:-Rocky-9-GenericCloud.latest.x86_64.qcow2}"
      export IMAGE_LINK="${IMAGE_LINK_ROCKY:-https://dl.rockylinux.org/pub/rocky/9/images/x86_64/Rocky-9-GenericCloud.latest.x86_64.qcow2}"
      # Kubernetes component versions for rocky
      export KUBERNETES_SHORT_VERSION="${KUBERNETES_SHORT_VERSION_ROCKY:-1.29}"
      export KUBERNETES_MEDIUM_VERSION="${KUBERNETES_MEDIUM_VERSION_ROCKY:-v1.29}"
      export KUBERNETES_VERSION="${KUBERNETES_MEDIUM_VERSION_ROCKY:-v1.29}"
      export KUBERNETES_LONG_VERSION="${KUBERNETES_LONG_VERSION_ROCKY:-1.29.6}"
      export CNI_PLUGINS_VERSION="${CNI_PLUGINS_VERSION_ROCKY:-v1.4.0}"
      export CALICO_VERSION="${CALICO_VERSION_ROCKY:-v3.26.4}"
      export METALLB_VERSION="${METALLB_VERSION_ROCKY:-v0.14.3}"
      export COREDNS_VERSION="${COREDNS_VERSION_ROCKY:-v1.10.1}"
      export METRICS_SERVER_VERSION="${METRICS_SERVER_VERSION_ROCKY:-v0.6.4}"
      export ETCD_VERSION="${ETCD_VERSION_ROCKY:-v3.5.10}"
      export KUBELET_SERVING_CERT_APPROVER_VERSION="${KUBELET_SERVING_CERT_APPROVER_VERSION_ROCKY:-v0.1.8}"
      export LOCAL_PATH_PROVISIONER_VERSION="${LOCAL_PATH_PROVISIONER_VERSION_ROCKY:-v0.0.24}"
      export CERT_MANAGER_VERSION="${CERT_MANAGER_VERSION_ROCKY:-v1.15.3}"
      export ARGOCD_VERSION="${ARGOCD_VERSION_ROCKY:-v2.12.3}"
      export INGRESS_NGINX_VERSION="${INGRESS_NGINX_VERSION_ROCKY:-v1.11.2}"
      ;;
    suse)
      # Template VM configuration
      export TEMPLATE_VM_ID="${TEMPLATE_VM_ID_SUSE:-941}"
      export TEMPLATE_VM_NAME="${TEMPLATE_VM_NAME_SUSE:-tpl-suse-15-k8s}"
      export IMAGE_NAME="${IMAGE_NAME_SUSE:-openSUSE-Leap-15.6-Minimal-VM.x86_64-15.6.0-Cloud-Build16.32.qcow2}"
      export IMAGE_LINK="${IMAGE_LINK_SUSE:-https://download.opensuse.org/distribution/leap/15.6/appliances/openSUSE-Leap-15.6-Minimal-VM.x86_64-15.6.0-Cloud-Build16.32.qcow2}"
      # Kubernetes component versions for suse
      export KUBERNETES_SHORT_VERSION="${KUBERNETES_SHORT_VERSION_SUSE:-1.30}"
      export KUBERNETES_MEDIUM_VERSION="${KUBERNETES_MEDIUM_VERSION_SUSE:-v1.30}"
      export KUBERNETES_VERSION="${KUBERNETES_MEDIUM_VERSION_SUSE:-v1.30}"
      export KUBERNETES_LONG_VERSION="${KUBERNETES_LONG_VERSION_SUSE:-1.30.8}"
      export CNI_PLUGINS_VERSION="${CNI_PLUGINS_VERSION_SUSE:-v1.4.0}"
      export CALICO_VERSION="${CALICO_VERSION_SUSE:-v3.27.0}"
      export METALLB_VERSION="${METALLB_VERSION_SUSE:-v0.14.5}"
      export COREDNS_VERSION="${COREDNS_VERSION_SUSE:-v1.11.1}"
      export METRICS_SERVER_VERSION="${METRICS_SERVER_VERSION_SUSE:-v0.7.1}"
      export ETCD_VERSION="${ETCD_VERSION_SUSE:-v3.5.12}"
      export KUBELET_SERVING_CERT_APPROVER_VERSION="${KUBELET_SERVING_CERT_APPROVER_VERSION_SUSE:-v0.1.9}"
      export LOCAL_PATH_PROVISIONER_VERSION="${LOCAL_PATH_PROVISIONER_VERSION_SUSE:-v0.0.26}"
      export CERT_MANAGER_VERSION="${CERT_MANAGER_VERSION_SUSE:-v1.16.2}"
      export ARGOCD_VERSION="${ARGOCD_VERSION_SUSE:-v2.13.2}"
      export INGRESS_NGINX_VERSION="${INGRESS_NGINX_VERSION_SUSE:-v1.12.0}"
      ;;
    *)
      echo -e "${YELLOW}Warning: Unknown workspace '$workspace'. Template variables not set.${ENDCOLOR}"
      echo -e "${BLUE}Available workspaces: debian, ubuntu, rocky, suse${ENDCOLOR}"
      return 1
      ;;
  esac
  
  echo -e "${BLUE}Set template variables for workspace '$workspace':${ENDCOLOR}"
  echo -e "${BLUE}  TEMPLATE_VM_ID: $TEMPLATE_VM_ID${ENDCOLOR}"
  echo -e "${BLUE}  TEMPLATE_VM_NAME: $TEMPLATE_VM_NAME${ENDCOLOR}"
  echo -e "${BLUE}  IMAGE_NAME: $IMAGE_NAME${ENDCOLOR}"
  echo -e "${BLUE}  KUBERNETES_VERSION: $KUBERNETES_MEDIUM_VERSION${ENDCOLOR}"
  echo -e "${BLUE}  CALICO_VERSION: $CALICO_VERSION${ENDCOLOR}"
  echo -e "${BLUE}  METALLB_VERSION: $METALLB_VERSION${ENDCOLOR}"
  echo -e "${BLUE}  COREDNS_VERSION: $COREDNS_VERSION${ENDCOLOR}"
  echo -e "${BLUE}  ETCD_VERSION: $ETCD_VERSION${ENDCOLOR}"
}
export -f set_workspace_template_vars

run_ansible_playbook() {
  local playbook_name="$1"
  local target_hosts="$2" # Optional: specific host or group for the playbook, defaults to 'all'
  local extra_vars_string="$3" # Optional: additional extra vars

  local repo_root
  repo_root=$(get_repo_path)
  local ansible_dir="$repo_root/ansible"
  local inventory_file="$ansible_dir/inventory/tofu_inventory.py" # Changed from terraform_inventory.py

  if [ ! -f "$inventory_file" ]; then
    echo -e "${RED}Error: Ansible inventory file not found at $inventory_file${ENDCOLOR}" >&2
    echo -e "${RED}Ensure Tofu has been run and the inventory script is in place.${ENDCOLOR}" >&2 # Changed from Terraform
    return 1
  fi

  if [ ! -x "$inventory_file" ]; then
    echo -e "${YELLOW}Warning: Inventory script $inventory_file is not executable. Attempting to chmod +x.${ENDCOLOR}"
    chmod +x "$inventory_file"
    if [ ! -x "$inventory_file" ]; then
        echo -e "${RED}Error: Failed to make inventory script $inventory_file executable.${ENDCOLOR}" >&2
        return 1
    fi
  fi

  local current_cluster
  current_cluster=$(get_current_cluster_context)

  # Base Ansible command
  # Ensure ansible_dir is quoted if it can contain spaces, though REPO_PATH usually doesn't.
  # Using ansible.cfg from the ansible_dir for settings like remote_user
  pushd "$ansible_dir" > /dev/null || { echo -e "${RED}Failed to change directory to $ansible_dir${ENDCOLOR}"; return 1; }

  local ansible_cmd="ansible-playbook -i $inventory_file playbooks/$playbook_name --ssh-extra-args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'"

  # Add common extra vars
  ansible_cmd+=" -e ansible_user=$(grep -Po '^remote_user\s*=\s*\K.*' ansible.cfg || echo 'root')" # Get user from ansible.cfg
  ansible_cmd+=" -e current_cluster_context=$current_cluster"
  # Remove 'v' prefix from kubernetes version if present, default to 1.31
  local k8s_version="${KUBERNETES_VERSION:-v1.31}"
  k8s_version="${k8s_version#v}"  # Remove 'v' prefix
  ansible_cmd+=" -e kubernetes_version=$k8s_version"
  ansible_cmd+=" -e kubernetes_patch_version=${kubernetes_patch_version:-latest}"


  # Add target hosts if specified
  if [ -n "$target_hosts" ] && [ "$target_hosts" != "all" ]; then
    ansible_cmd+=" --limit $target_hosts"
  fi

  # Add any other specific extra vars passed to the function
  if [ -n "$extra_vars_string" ]; then
    ansible_cmd+=" $extra_vars_string"
  fi

  echo -e "${BLUE}Running: $ansible_cmd${ENDCOLOR}"
  eval "$ansible_cmd" # Use eval to correctly interpret quotes in extra_vars_string if any
  local exit_code=$?

  popd > /dev/null || return 1

  if [ $exit_code -ne 0 ]; then
    echo -e "${RED}Error: Ansible playbook $playbook_name failed with exit code $exit_code.${ENDCOLOR}" >&2
    return 1
  fi
  return 0
}
export -f run_ansible_playbook

display_usage() {
  echo "Usage: cpc <command> [options]"
  echo ""
  echo "Commands:"
  echo "  setup-cpc                      Initial setup for cpc command."
  echo "  ctx [<cluster_name>]           Get or set the current cluster context (Tofu workspace)."
  echo "  template                       Creates a VM template for Kubernetes"
  echo "  bootstrap                      Bootstrap a complete Kubernetes cluster on deployed VMs"
  echo "  clear-ssh-hosts                Clear VM IP addresses from ~/.ssh/known_hosts"
  echo "  clear-ssh-maps                 Clear SSH control sockets and connections for VMs"
  echo "  load_secrets                   Load and display secrets from SOPS configuration"
  echo "  update-pihole <action>         Manage Pi-hole DNS records. Action can be 'add' or 'unregister-dns'." # Changed
  echo "  generate-hostnames             Generate hostname configurations for VMs in Proxmox"
  echo "  scripts/<script_name>          Run any script from the scripts directory"
  echo "  deploy <tofu_cmd> [opts]       Run any 'tofu' command (e.g., plan, apply, output) in context."
  echo "  start-vms                      Start all VMs in the current context."
  echo "  stop-vms                       Stop all VMs in the current context."
  echo ""
  echo "  get-kubeconfig                 Retrieve and merge Kubernetes cluster config into local kubeconfig."
  echo "  add-nodes                      Add new worker nodes to the cluster."
  echo "  drain-node <node_name>         Drain workloads from a node."
  echo "  delete-node <node_name>        Delete a node from the Kubernetes cluster."
  echo "  upgrade-node <node_name>       Upgrade Kubernetes on a specific node."
  echo "  reset-node <node_name>         Reset Kubernetes on a specific node."
  echo "  reset-all-nodes                Reset Kubernetes on all nodes in the current context."
  echo "  upgrade-addons                 Install/upgrade cluster addons with interactive menu (CNI, MetalLB, cert-manager, ArgoCD, etc.)."
  echo "  upgrade-k8s                    Upgrade Kubernetes control plane."
  echo "  vmctl                          (Placeholder) Suggests using Tofu for VM control." # Changed from Terraform
  echo "  run-command <target> \"<cmd>\"   Run a shell command on target host(s) or group."
  echo ""
  echo "Use 'cpc <command> --help' for more details on a specific command." # Changed from ccr
}

# --- Main Script Logic ---

# Ensure config directory exists
mkdir -p "$CONFIG_DIR"

# Check for essential commands early
check_required_commands "ansible-playbook" "ansible-inventory" "tofu" "kubectl" "jq"

COMMAND="$1"
shift # Remove command from arguments, rest are options

# Load REPO_PATH if not doing setup
if [[ "$COMMAND" != "setup-cpc" && "$COMMAND" != "" && "$COMMAND" != "-h" && "$COMMAND" != "--help" ]]; then # Changed from setup-ccr
  REPO_PATH=$(get_repo_path)
  export REPO_PATH
  # Load environment variables from cpc.env
  load_env_vars # Will now use CPC_ENV_FILE
fi


case "$COMMAND" in
  setup-cpc) # Changed from setup-ccr
    current_script_path="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    echo "$current_script_path" > "$REPO_PATH_FILE"
    echo -e "${GREEN}cpc setup complete. Repository path set to: $current_script_path${ENDCOLOR}" # Changed from ccr
    echo -e "${BLUE}You might want to add this script to your PATH, e.g., by creating a symlink in /usr/local/bin/cpc${ENDCOLOR}" # Changed from ccr
    echo -e "${BLUE}Example: sudo ln -s "$current_script_path/cpc" /usr/local/bin/cpc${ENDCOLOR}" # Changed from ccr
    echo -e "${BLUE}Also, create a 'cpc.env' file in '$current_script_path' for version management (see cpc.env.example).${ENDCOLOR}" # Changed from ccr.env and ccr.env.example
    ;;

  get-kubeconfig)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc get-kubeconfig [--context-name <name>] [--force]"
      echo ""
      echo "Retrieves the Kubernetes cluster configuration from the control plane node"
      echo "and merges it into your local kubeconfig file (~/.kube/config)."
      echo ""
      echo "Options:"
      echo "  --context-name <name>  Set a custom context name (default: cluster-<cpc_context>)"
      echo "  --force               (Optional) Explicit flag to confirm overwriting existing context"
      echo ""
      echo "Note: Existing contexts with the same name will be automatically overwritten."
      echo ""
      echo "The command will:"
      echo "  1. Connect to the control plane node and retrieve /etc/kubernetes/admin.conf"
      echo "  2. Update server endpoint to use the control plane node's IP"
      echo "  3. Merge the configuration into your local kubeconfig"
      echo "  4. Set the new context as the current context"
      exit 0
    fi

    # Parse command line arguments
    custom_context_name=""
    force_overwrite=false
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --context-name)
          custom_context_name="$2"
          shift 2
          ;;
        --force)
          force_overwrite=true
          shift
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          exit 1
          ;;
      esac
    done

    # Get current cluster context and repo path
    current_ctx=$(get_current_cluster_context)
    repo_root=$(get_repo_path)
    
    # Set context name
    if [ -z "$custom_context_name" ]; then
      context_name="cluster-${current_ctx}"
    else
      context_name="$custom_context_name"
    fi

    echo -e "${BLUE}Retrieving kubeconfig for cluster context: $current_ctx${ENDCOLOR}"
    echo -e "${BLUE}Kubernetes context will be named: $context_name${ENDCOLOR}"

    # Warn if context already exists
    if kubectl config get-contexts -o name | grep -q "^${context_name}$"; then
      if [ "$force_overwrite" = false ]; then
        echo -e "${YELLOW}Context '$context_name' already exists and will be overwritten.${ENDCOLOR}"
        echo -e "${YELLOW}Use --context-name to use a different name if desired.${ENDCOLOR}"
      else
        echo -e "${BLUE}Context '$context_name' exists and will be overwritten (--force specified).${ENDCOLOR}"
      fi
    fi

    # Get control plane IP from Terraform/Tofu output
    pushd "$repo_root/terraform" > /dev/null || { echo -e "${RED}Failed to change to terraform directory${ENDCOLOR}"; exit 1; }
    
    # Ensure we're in the correct workspace
    if ! tofu workspace select "$current_ctx" &>/dev/null; then
      echo -e "${RED}Failed to select Tofu workspace '$current_ctx'${ENDCOLOR}" >&2
      popd > /dev/null
      exit 1
    fi

    # Get control plane node IP
    control_plane_ip=$(tofu output -json k8s_node_ips 2>/dev/null | jq -r 'to_entries[] | select(.key | contains("controlplane")) | .value')
    if [ -z "$control_plane_ip" ] || [ "$control_plane_ip" = "null" ]; then
      echo -e "${RED}Failed to get control plane IP from Tofu output${ENDCOLOR}" >&2
      echo -e "${RED}Make sure the cluster is deployed and 'tofu output k8s_node_ips' returns valid data${ENDCOLOR}" >&2
      popd > /dev/null
      exit 1
    fi

    popd > /dev/null

    echo -e "${BLUE}Control plane IP: $control_plane_ip${ENDCOLOR}"

    # Create temporary directory for kubeconfig operations
    temp_dir=$(mktemp -d)
    temp_kubeconfig="$temp_dir/admin.conf"
    
    # Cleanup function
    cleanup_temp() {
      rm -rf "$temp_dir"
    }
    trap cleanup_temp EXIT

    # Get ansible config to determine remote user
    ansible_dir="$repo_root/ansible"
    remote_user=$(grep -Po '^remote_user\s*=\s*\K.*' "$ansible_dir/ansible.cfg" 2>/dev/null || echo 'root')

    echo -e "${BLUE}Retrieving kubeconfig from control plane node...${ENDCOLOR}"
    
    # Copy kubeconfig from control plane node using sudo
    if ! ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
         "${remote_user}@${control_plane_ip}" \
         "sudo cat /etc/kubernetes/admin.conf" > "$temp_kubeconfig" 2>/dev/null; then
      echo -e "${RED}Failed to retrieve kubeconfig from control plane node${ENDCOLOR}" >&2
      echo -e "${RED}Make sure you can SSH to $control_plane_ip as user $remote_user and use sudo${ENDCOLOR}" >&2
      exit 1
    fi

    # Update server endpoint in the kubeconfig
    if ! sed -i "s|server: https://.*:6443|server: https://${control_plane_ip}:6443|g" "$temp_kubeconfig"; then
      echo -e "${RED}Failed to update server endpoint in kubeconfig${ENDCOLOR}" >&2
      exit 1
    fi

    # Update context and user names to avoid conflicts
    if ! sed -i "s|kubernetes-admin@kubernetes|${context_name}|g" "$temp_kubeconfig"; then
      echo -e "${RED}Failed to update context name in kubeconfig${ENDCOLOR}" >&2
      exit 1
    fi

    if ! sed -i "s|name: kubernetes-admin|name: ${context_name}-admin|g" "$temp_kubeconfig"; then
      echo -e "${RED}Failed to update user name in kubeconfig${ENDCOLOR}" >&2
      exit 1
    fi

    if ! sed -i "s|name: kubernetes|name: ${context_name}-cluster|g" "$temp_kubeconfig"; then
      echo -e "${RED}Failed to update cluster name in kubeconfig${ENDCOLOR}" >&2
      exit 1
    fi

    if ! sed -i "s|cluster: kubernetes|cluster: ${context_name}-cluster|g" "$temp_kubeconfig"; then
      echo -e "${RED}Failed to update cluster reference in kubeconfig${ENDCOLOR}" >&2
      exit 1
    fi

    if ! sed -i "s|user: kubernetes-admin|user: ${context_name}-admin|g" "$temp_kubeconfig"; then
      echo -e "${RED}Failed to update user reference in kubeconfig${ENDCOLOR}" >&2
      exit 1
    fi

    # Merge the kubeconfig
    echo -e "${BLUE}Merging kubeconfig into ~/.kube/config...${ENDCOLOR}"
    
    # Backup existing kubeconfig if it exists
    if [ -f ~/.kube/config ]; then
      cp ~/.kube/config ~/.kube/config.backup.$(date +%Y%m%d_%H%M%S)
      echo -e "${BLUE}Existing kubeconfig backed up${ENDCOLOR}"
    fi

    # Ensure .kube directory exists
    mkdir -p ~/.kube

    # Merge kubeconfig
    if [ -f ~/.kube/config ] && [ -s ~/.kube/config ]; then
      # Existing config exists and is not empty
      
      # Remove existing context if it exists to avoid conflicts
      if kubectl config get-contexts -o name | grep -q "^${context_name}$"; then
        echo -e "${BLUE}Removing existing context '$context_name' to avoid conflicts...${ENDCOLOR}"
        kubectl config delete-context "$context_name" &>/dev/null || true
        kubectl config delete-cluster "${context_name}-cluster" &>/dev/null || true
        kubectl config delete-user "${context_name}-admin" &>/dev/null || true
      fi
      
      KUBECONFIG=~/.kube/config:$temp_kubeconfig kubectl config view --flatten > ~/.kube/config.tmp
      if [ -s ~/.kube/config.tmp ]; then
        mv ~/.kube/config.tmp ~/.kube/config
      else
        echo -e "${YELLOW}Warning: Merge resulted in empty config, using new config only${ENDCOLOR}"
        cp "$temp_kubeconfig" ~/.kube/config
      fi
    else
      # No existing config or empty config
      cp "$temp_kubeconfig" ~/.kube/config
    fi

    # Set the new context as current
    if kubectl config use-context "$context_name" &>/dev/null; then
      echo -e "${GREEN}Successfully set '$context_name' as the current context${ENDCOLOR}"
    else
      echo -e "${YELLOW}Kubeconfig merged but failed to set '$context_name' as current context${ENDCOLOR}"
      echo -e "${YELLOW}You can manually switch using: kubectl config use-context $context_name${ENDCOLOR}"
    fi

    # Test the connection
    echo -e "${BLUE}Testing connection to cluster...${ENDCOLOR}"
    if kubectl cluster-info --context "$context_name" &>/dev/null; then
      echo -e "${GREEN}Successfully connected to Kubernetes cluster!${ENDCOLOR}"
      echo -e "${BLUE}Cluster information:${ENDCOLOR}"
      kubectl cluster-info --context "$context_name"
    else
      echo -e "${YELLOW}Kubeconfig retrieved but connection test failed${ENDCOLOR}"
      echo -e "${YELLOW}Please check cluster status and network connectivity${ENDCOLOR}"
    fi

    echo -e "${GREEN}Kubeconfig setup completed for context: $context_name${ENDCOLOR}"
    ;;

  ctx)
    if [ -z "$1" ]; then
      current_ctx=$(get_current_cluster_context)
      echo "Current cluster context: $current_ctx"
      echo "Available Tofu workspaces:" # Changed from Terraform
      (cd "$REPO_PATH/terraform" && tofu workspace list) # Changed from terraform
      exit 0
    elif [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc ctx [<cluster_name>]" # Changed from ccr
      echo "Sets the current cluster context for cpc and switches Tofu workspace." # Changed from ccr and Terraform
      exit 0
    fi
    cluster_name="$1"
    echo "$cluster_name" > "$CLUSTER_CONTEXT_FILE"
    echo -e "${GREEN}Cluster context set to: $cluster_name${ENDCOLOR}"
    pushd "$REPO_PATH/terraform" > /dev/null || exit 1
    if tofu workspace list | grep -qw "$cluster_name"; then # Changed from terraform
      tofu workspace select "$cluster_name" # Changed from terraform
    else
      echo -e "${YELLOW}Tofu workspace '$cluster_name' does not exist. Creating and selecting.${ENDCOLOR}" # Changed from Terraform
      tofu workspace new "$cluster_name" # Changed from terraform
    fi
    popd > /dev/null || exit 1
    
    # Update template variables for the new workspace context
    set_workspace_template_vars "$cluster_name"
    ;;
  
  template)
    # Ensure workspace-specific template variables are set
    current_ctx=$(get_current_cluster_context)
    set_workspace_template_vars "$current_ctx"
    
    # Check if essential template variables are set
    if [ -z "$TEMPLATE_VM_ID" ] || [ -z "$TEMPLATE_VM_NAME" ] || [ -z "$IMAGE_NAME" ] || [ -z "$IMAGE_LINK" ]; then
      echo -e "${RED}Error: Template variables not properly set for workspace '$current_ctx'.${ENDCOLOR}"
      echo -e "${RED}Please ensure cpc.env contains the required TEMPLATE_VM_ID_*, TEMPLATE_VM_NAME_*, IMAGE_NAME_*, IMAGE_LINK_* variables.${ENDCOLOR}"
      exit 1
    fi
    
    (
      "$REPO_PATH/scripts/template.sh" "$@"
    )
    ;;

  load_secrets)
    echo -e "${BLUE}Loading secrets from SOPS...${ENDCOLOR}"
    load_secrets
    echo -e "${GREEN}Secrets loaded successfully!${ENDCOLOR}"
    echo -e "${BLUE}Available variables:${ENDCOLOR}"
    echo -e "${BLUE}  PROXMOX_HOST: $PROXMOX_HOST${ENDCOLOR}"
    echo -e "${BLUE}  PROXMOX_USERNAME: $PROXMOX_USERNAME${ENDCOLOR}"
    echo -e "${BLUE}  VM_USERNAME: $VM_USERNAME${ENDCOLOR}"
    echo -e "${BLUE}  VM_SSH_KEY: ${VM_SSH_KEY:0:20}...${ENDCOLOR}"
    ;;

  bootstrap)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc bootstrap [--skip-check] [--force]"
      echo ""
      echo "Bootstrap a complete Kubernetes cluster on the deployed VMs."
      echo ""
      echo "The bootstrap process includes:"
      echo "  1. Install Kubernetes components (kubelet, kubeadm, kubectl, containerd)"
      echo "  2. Initialize control plane with kubeadm"
      echo "  3. Install Calico CNI plugin"
      echo "  4. Join worker nodes to the cluster"
      echo "  5. Configure kubectl access for the cluster"
      echo ""
      echo "Options:"
      echo "  --skip-check   Skip VM connectivity check before starting"
      echo "  --force        Force bootstrap even if cluster appears already initialized"
      echo ""
      echo "Prerequisites:"
      echo "  - VMs must be deployed and accessible (use 'cpc deploy apply')"
      echo "  - SSH access configured to all nodes"
      echo "  - SOPS secrets loaded for VM authentication"
      echo ""
      echo "Example workflow:"
      echo "  cpc ctx ubuntu           # Set context"
      echo "  cpc deploy apply         # Deploy VMs"
      echo "  cpc bootstrap           # Bootstrap Kubernetes cluster"
      echo "  cpc get-kubeconfig      # Get cluster access"
      exit 0
    fi

    # Parse command line arguments
    skip_check=false
    force_bootstrap=false
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --skip-check)
          skip_check=true
          shift
          ;;
        --force)
          force_bootstrap=true
          shift
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          exit 1
          ;;
      esac
    done

    # Check if secrets are loaded
    check_secrets_loaded

    current_ctx=$(get_current_cluster_context)
    repo_root=$(get_repo_path)
    echo -e "${BLUE}Starting Kubernetes bootstrap for context '$current_ctx'...${ENDCOLOR}"

    # Verify that VMs are deployed and accessible
    if [ "$skip_check" = false ]; then
      echo -e "${BLUE}Checking VM connectivity...${ENDCOLOR}"
      pushd "$repo_root/terraform" > /dev/null || { echo -e "${RED}Failed to change to terraform directory${ENDCOLOR}"; exit 1; }
      
      # Check if we're in the right workspace
      if ! tofu workspace select "$current_ctx" &>/dev/null; then
        echo -e "${RED}Failed to select Tofu workspace '$current_ctx'${ENDCOLOR}" >&2
        echo -e "${RED}Please ensure VMs are deployed with 'cpc deploy apply'${ENDCOLOR}" >&2
        popd > /dev/null
        exit 1
      fi

      # Check if VMs exist
      vm_ips=$(tofu output -json k8s_node_ips 2>/dev/null)
      if [ -z "$vm_ips" ] || [ "$vm_ips" = "null" ]; then
        echo -e "${RED}No VMs found in Tofu output. Please deploy VMs first with 'cpc deploy apply'${ENDCOLOR}" >&2
        popd > /dev/null
        exit 1
      fi

      popd > /dev/null

      echo -e "${GREEN}VM connectivity check passed${ENDCOLOR}"
    fi

    # Check if cluster is already initialized (unless forced)
    if [ "$force_bootstrap" = false ]; then
      echo -e "${BLUE}Checking if cluster is already initialized...${ENDCOLOR}"
      
      # Try to connect to potential control plane and check if Kubernetes is running
      repo_root=$(get_repo_path)
      pushd "$repo_root/terraform" > /dev/null || exit 1
      control_plane_ip=$(tofu output -json k8s_node_ips 2>/dev/null | jq -r 'to_entries[] | select(.key | contains("controlplane")) | .value' | head -1)
      popd > /dev/null
      
      if [ -n "$control_plane_ip" ] && [ "$control_plane_ip" != "null" ]; then
        # Check if kubeconfig exists on control plane
        ansible_dir="$repo_root/ansible"
        remote_user=$(grep -Po '^remote_user\s*=\s*\K.*' "$ansible_dir/ansible.cfg" 2>/dev/null || echo 'root')
        
        if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o UserKnownHostsFile=/dev/null \
             "${remote_user}@${control_plane_ip}" \
             "test -f /etc/kubernetes/admin.conf" 2>/dev/null; then
          echo -e "${YELLOW}Kubernetes cluster appears to already be initialized on $control_plane_ip${ENDCOLOR}"
          echo -e "${YELLOW}Use --force to bootstrap anyway (this will reset the cluster)${ENDCOLOR}"
          exit 1
        fi
      fi
    fi

    # Run the bootstrap playbooks
    echo -e "${GREEN}Starting Kubernetes cluster bootstrap...${ENDCOLOR}"
    ansible_dir="$repo_root/ansible"
    inventory_file="$ansible_dir/inventory/tofu_inventory.py"

    # Check if inventory exists
    if [ ! -f "$inventory_file" ]; then
      echo -e "${RED}Ansible inventory not found at $inventory_file${ENDCOLOR}" >&2
      exit 1
    fi

    # First, verify connectivity to all nodes
    echo -e "${BLUE}Testing Ansible connectivity to all nodes...${ENDCOLOR}"
    if ! ansible all -i "$inventory_file" -m ping --ssh-extra-args="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"; then
      echo -e "${RED}Failed to connect to all nodes via Ansible${ENDCOLOR}" >&2
      echo -e "${RED}Please check SSH access and ensure VMs are running${ENDCOLOR}" >&2
      exit 1
    fi

    echo -e "${GREEN}Ansible connectivity test passed${ENDCOLOR}"

    # Step 1: Install Kubernetes components
    echo -e "${BLUE}Step 1: Installing Kubernetes components (kubelet, kubeadm, kubectl, containerd)...${ENDCOLOR}"
    if ! run_ansible_playbook "install_kubernetes_cluster.yml" "all" ""; then
      echo -e "${RED}Failed to install Kubernetes components${ENDCOLOR}" >&2
      exit 1
    fi

    # Step 2: Initialize cluster and setup CNI
    echo -e "${BLUE}Step 2: Initializing Kubernetes cluster and installing Calico CNI...${ENDCOLOR}"
    if ! run_ansible_playbook "initialize_kubernetes_cluster.yml" "all" ""; then
      echo -e "${RED}Failed to initialize Kubernetes cluster${ENDCOLOR}" >&2
      exit 1
    fi

    # Step 3: Validate cluster
    echo -e "${BLUE}Step 3: Validating cluster installation...${ENDCOLOR}"
    if ! run_ansible_playbook "validate_cluster.yml" "control_plane" ""; then
      echo -e "${YELLOW}Cluster validation failed, but continuing...${ENDCOLOR}"
    fi

    echo -e "${GREEN}Kubernetes cluster bootstrap completed successfully!${ENDCOLOR}"
    echo -e "${BLUE}Next steps:${ENDCOLOR}"
    echo -e "${BLUE}  1. Get cluster access: cpc get-kubeconfig${ENDCOLOR}"
    echo -e "${BLUE}  2. Install addons: cpc upgrade-addons${ENDCOLOR}"
    echo -e "${BLUE}  3. Verify cluster: kubectl get nodes -o wide${ENDCOLOR}"
    ;;

  clear-ssh-hosts)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc clear-ssh-hosts [--all] [--dry-run]"
      echo ""
      echo "Clear VM IP addresses from ~/.ssh/known_hosts to resolve SSH key conflicts"
      echo "when VMs are recreated with the same IP addresses but new SSH keys."
      echo ""
      echo "Options:"
      echo "  --all       Clear all VM IPs from all contexts (not just current)"
      echo "  --dry-run   Show what would be removed without actually removing"
      echo ""
      echo "The command will:"
      echo "  1. Get VM IP addresses from current Terraform/Tofu outputs"
      echo "  2. Remove matching entries from ~/.ssh/known_hosts"
      echo "  3. Display summary of removed entries"
      echo ""
      echo "Example usage:"
      echo "  cpc clear-ssh-hosts           # Clear IPs from current context"
      echo "  cpc clear-ssh-hosts --all     # Clear IPs from all contexts"
      echo "  cpc clear-ssh-hosts --dry-run # Preview what would be removed"
      exit 0
    fi

    # Parse command line arguments
    clear_all=false
    dry_run=false
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --all)
          clear_all=true
          shift
          ;;
        --dry-run)
          dry_run=true
          shift
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          echo "Use 'cpc clear-ssh-hosts --help' for usage information."
          exit 1
          ;;
      esac
    done

    # Check if ~/.ssh/known_hosts exists
    if [ ! -f ~/.ssh/known_hosts ]; then
      echo -e "${YELLOW}No ~/.ssh/known_hosts file found. Nothing to clear.${ENDCOLOR}"
      exit 0
    fi

    current_ctx=$(get_current_cluster_context)
    repo_root=$(get_repo_path)
    
    echo -e "${BLUE}Clearing SSH known_hosts entries for VM IP addresses...${ENDCOLOR}"
    
    # Function to get VM IPs from a specific context
    get_vm_ips_from_context() {
      local context="$1"
      local terraform_dir="$repo_root/terraform"
      
      # Change to terraform directory and select workspace
      pushd "$terraform_dir" > /dev/null || return 1
      
      # Save current workspace before switching
      local original_workspace
      original_workspace=$(tofu workspace show 2>/dev/null)
      
      if ! tofu workspace select "$context" &>/dev/null; then
        echo -e "${YELLOW}Warning: Could not select Tofu workspace '$context'${ENDCOLOR}" >&2
        popd > /dev/null
        return 1
      fi
      
      # Get VM IPs from Tofu output
      local vm_ips
      vm_ips=$(tofu output -json k8s_node_ips 2>/dev/null)
      
      # Restore original workspace
      if [ -n "$original_workspace" ] && [ "$original_workspace" != "$context" ]; then
        tofu workspace select "$original_workspace" &>/dev/null
      fi
      
      popd > /dev/null
      
      if [ -z "$vm_ips" ] || [ "$vm_ips" = "null" ] || [ "$vm_ips" = "{}" ]; then
        return 1
      fi
      
      # Extract IP addresses from JSON output
      echo "$vm_ips" | jq -r 'to_entries[] | .value' 2>/dev/null | grep -E '^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$'
    }
    
    # Collect all VM IPs to remove
    vm_ips_to_clear=()
    
    if [ "$clear_all" = true ]; then
      echo -e "${BLUE}Collecting VM IPs from all contexts...${ENDCOLOR}"
      
      # Get all available workspaces
      pushd "$repo_root/terraform" > /dev/null || { echo -e "${RED}Failed to access terraform directory${ENDCOLOR}"; exit 1; }
      workspaces=$(tofu workspace list | grep -v '^\*' | sed 's/^[ *]*//' | grep -v '^default$')
      popd > /dev/null
      
      for workspace in $workspaces; do
        echo -e "${BLUE}  Checking context: $workspace${ENDCOLOR}"
        ips=$(get_vm_ips_from_context "$workspace")
        if [ -n "$ips" ]; then
          while IFS= read -r ip; do
            if [ -n "$ip" ]; then
              vm_ips_to_clear+=("$ip")
            fi
          done <<< "$ips"
          echo -e "${BLUE}    Found IPs: $(echo "$ips" | tr '\n' ' ')${ENDCOLOR}"
        else
          echo -e "${YELLOW}    No VMs found in context '$workspace'${ENDCOLOR}"
        fi
      done
    else
      echo -e "${BLUE}Collecting VM IPs from current context: $current_ctx${ENDCOLOR}"
      ips=$(get_vm_ips_from_context "$current_ctx")
      if [ -n "$ips" ]; then
        while IFS= read -r ip; do
          if [ -n "$ip" ]; then
            vm_ips_to_clear+=("$ip")
          fi
        done <<< "$ips"
        echo -e "${BLUE}  Found IPs: $(echo "$ips" | tr '\n' ' ')${ENDCOLOR}"
      else
        echo -e "${YELLOW}No VMs found in current context '$current_ctx'${ENDCOLOR}"
        echo -e "${BLUE}Make sure VMs are deployed with 'cpc deploy apply'${ENDCOLOR}"
        exit 0
      fi
    fi
    
    # Remove duplicates
    vm_ips_to_clear=($(printf '%s\n' "${vm_ips_to_clear[@]}" | sort -u))
    
    if [ ${#vm_ips_to_clear[@]} -eq 0 ]; then
      echo -e "${YELLOW}No VM IP addresses found to clear.${ENDCOLOR}"
      exit 0
    fi
    
    echo -e "${BLUE}VM IP addresses to clear from ~/.ssh/known_hosts:${ENDCOLOR}"
    for ip in "${vm_ips_to_clear[@]}"; do
      echo -e "${BLUE}  - $ip${ENDCOLOR}"
    done
    
    if [ "$dry_run" = true ]; then
      echo -e "${YELLOW}Dry run mode - showing what would be removed:${ENDCOLOR}"
      for ip in "${vm_ips_to_clear[@]}"; do
        entries=$(grep -n "^$ip " ~/.ssh/known_hosts 2>/dev/null || true)
        if [ -n "$entries" ]; then
          echo -e "${YELLOW}  Would remove entries for $ip:${ENDCOLOR}"
          echo "$entries" | sed 's/^/    /'
        else
          echo -e "${BLUE}  No entries found for $ip${ENDCOLOR}"
        fi
      done
      echo -e "${BLUE}Run without --dry-run to actually remove entries.${ENDCOLOR}"
      exit 0
    fi
    
    # Create backup of known_hosts
    backup_file=~/.ssh/known_hosts.backup.$(date +%Y%m%d_%H%M%S)
    cp ~/.ssh/known_hosts "$backup_file"
    echo -e "${BLUE}Created backup: $backup_file${ENDCOLOR}"
    
    # Remove entries for each IP
    removed_count=0
    for ip in "${vm_ips_to_clear[@]}"; do
      # Count entries before removal
      before_count=$(grep -c "^$ip " ~/.ssh/known_hosts 2>/dev/null || echo "0")
      
      if [ "$before_count" -gt 0 ]; then
        # Remove entries for this IP
        sed -i "/^$ip /d" ~/.ssh/known_hosts
        
        # Count entries after removal
        after_count=$(grep -c "^$ip " ~/.ssh/known_hosts 2>/dev/null || echo "0")
        entries_removed=$((before_count - after_count))
        
        if [ $entries_removed -gt 0 ]; then
          echo -e "${GREEN}  Removed $entries_removed entries for $ip${ENDCOLOR}"
          removed_count=$((removed_count + entries_removed))
        fi
      else
        echo -e "${BLUE}  No entries found for $ip${ENDCOLOR}"
      fi
    done
    
    if [ $removed_count -gt 0 ]; then
      echo -e "${GREEN}Successfully removed $removed_count SSH known_hosts entries.${ENDCOLOR}"
      echo -e "${BLUE}Backup saved to: $backup_file${ENDCOLOR}"
    else
      echo -e "${YELLOW}No SSH known_hosts entries were removed.${ENDCOLOR}"
      # Remove backup if nothing was changed
      rm -f "$backup_file"
    fi
    
    echo -e "${BLUE}SSH known_hosts cleanup completed.${ENDCOLOR}"
    ;;

  clear-ssh-maps)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc clear-ssh-maps [--all] [--dry-run]"
      echo ""
      echo "Clear SSH control sockets and connections for VMs to resolve SSH connection issues"
      echo "when VMs are recreated or SSH configurations change."
      echo ""
      echo "Options:"
      echo "  --all       Clear SSH connections for all contexts (not just current)"
      echo "  --dry-run   Show what would be cleared without actually clearing"
      echo ""
      echo "The command will:"
      echo "  1. Get VM IP addresses from current Terraform/Tofu outputs"
      echo "  2. Close active SSH connections to those IPs"
      echo "  3. Remove SSH control sockets (if ControlMaster is enabled)"
      echo "  4. Display summary of cleared connections"
      echo ""
      echo "Example usage:"
      echo "  cpc clear-ssh-maps           # Clear SSH connections for current context"
      echo "  cpc clear-ssh-maps --all     # Clear SSH connections for all contexts"
      echo "  cpc clear-ssh-maps --dry-run # Preview what would be cleared"
      exit 0
    fi

    # Parse command line arguments
    clear_all=false
    dry_run=false
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --all)
          clear_all=true
          shift
          ;;
        --dry-run)
          dry_run=true
          shift
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          echo "Use 'cpc clear-ssh-maps --help' for usage information."
          exit 1
          ;;
      esac
    done

    current_ctx=$(get_current_cluster_context)
    repo_root=$(get_repo_path)
    
    echo -e "${BLUE}Clearing SSH connections and control sockets for VM IP addresses...${ENDCOLOR}"
    
    # Function to get VM IPs from a specific context (reuse from clear-ssh-hosts)
    get_vm_ips_from_context() {
      local context="$1"
      local terraform_dir="$repo_root/terraform"
      
      # Change to terraform directory and select workspace
      pushd "$terraform_dir" > /dev/null || return 1
      
      # Save current workspace before switching
      local original_workspace
      original_workspace=$(tofu workspace show 2>/dev/null)
      
      if ! tofu workspace select "$context" &>/dev/null; then
        echo -e "${YELLOW}Warning: Could not select Tofu workspace '$context'${ENDCOLOR}" >&2
        popd > /dev/null
        return 1
      fi
      
      # Get VM IPs from Tofu output
      local vm_ips
      vm_ips=$(tofu output -json k8s_node_ips 2>/dev/null)
      
      # Restore original workspace
      if [ -n "$original_workspace" ] && [ "$original_workspace" != "$context" ]; then
        tofu workspace select "$original_workspace" &>/dev/null
      fi
      
      popd > /dev/null
      
      if [ -z "$vm_ips" ] || [ "$vm_ips" = "null" ] || [ "$vm_ips" = "{}" ]; then
        return 1
      fi
      
      # Extract IP addresses from JSON output
      echo "$vm_ips" | jq -r 'to_entries[] | .value' 2>/dev/null | grep -E '^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$'
    }
    
    # Collect all VM IPs to clear
    vm_ips_to_clear=()
    
    if [ "$clear_all" = true ]; then
      echo -e "${BLUE}Collecting VM IPs from all contexts...${ENDCOLOR}"
      
      # Get all available workspaces
      pushd "$repo_root/terraform" > /dev/null || { echo -e "${RED}Failed to access terraform directory${ENDCOLOR}"; exit 1; }
      workspaces=$(tofu workspace list | grep -v '^\*' | sed 's/^[ *]*//' | grep -v '^default$')
      popd > /dev/null
      
      for workspace in $workspaces; do
        echo -e "${BLUE}  Checking context: $workspace${ENDCOLOR}"
        ips=$(get_vm_ips_from_context "$workspace")
        if [ -n "$ips" ]; then
          while IFS= read -r ip; do
            if [ -n "$ip" ]; then
              vm_ips_to_clear+=("$ip")
            fi
          done <<< "$ips"
          echo -e "${BLUE}    Found IPs: $(echo "$ips" | tr '\n' ' ')${ENDCOLOR}"
        else
          echo -e "${YELLOW}    No VMs found in context '$workspace'${ENDCOLOR}"
        fi
      done
    else
      echo -e "${BLUE}Collecting VM IPs from current context: $current_ctx${ENDCOLOR}"
      ips=$(get_vm_ips_from_context "$current_ctx")
      if [ -n "$ips" ]; then
        while IFS= read -r ip; do
          if [ -n "$ip" ]; then
            vm_ips_to_clear+=("$ip")
          fi
        done <<< "$ips"
        echo -e "${BLUE}  Found IPs: $(echo "$ips" | tr '\n' ' ')${ENDCOLOR}"
      else
        echo -e "${YELLOW}No VMs found in current context '$current_ctx'${ENDCOLOR}"
        echo -e "${BLUE}Make sure VMs are deployed with 'cpc deploy apply'${ENDCOLOR}"
        exit 0
      fi
    fi
    
    # Remove duplicates
    vm_ips_to_clear=($(printf '%s\n' "${vm_ips_to_clear[@]}" | sort -u))
    
    if [ ${#vm_ips_to_clear[@]} -eq 0 ]; then
      echo -e "${YELLOW}No VM IP addresses found to clear SSH connections for.${ENDCOLOR}"
      exit 0
    fi
    
    echo -e "${BLUE}VM IP addresses to clear SSH connections for:${ENDCOLOR}"
    for ip in "${vm_ips_to_clear[@]}"; do
      echo -e "${BLUE}  - $ip${ENDCOLOR}"
    done
    
    # Find SSH control sockets and active connections
    connections_found=0
    sockets_found=0
    
    if [ "$dry_run" = true ]; then
      echo -e "${YELLOW}Dry run mode - showing what would be cleared:${ENDCOLOR}"
      
      for ip in "${vm_ips_to_clear[@]}"; do
        echo -e "${BLUE}  Checking SSH connections for $ip:${ENDCOLOR}"
        
        # Check for active SSH connections
        active_connections=$(ps aux | grep -E "ssh.*$ip" | grep -v grep | grep -v "clear-ssh-maps" || true)
        if [ -n "$active_connections" ]; then
          echo -e "${YELLOW}    Active SSH connections found:${ENDCOLOR}"
          echo "$active_connections" | sed 's/^/      /'
          connections_found=$((connections_found + 1))
        fi
        
        # Check for SSH control sockets in common locations
        socket_locations=(
          "$HOME/.ssh/sockets"
          "$HOME/.ssh/connections"
          "$HOME/.ssh/master"
          "/tmp"
        )
        
        for socket_dir in "${socket_locations[@]}"; do
          if [ -d "$socket_dir" ]; then
            sockets=$(find "$socket_dir" -name "*$ip*" -type s 2>/dev/null || true)
            if [ -n "$sockets" ]; then
              echo -e "${YELLOW}    SSH control sockets found in $socket_dir:${ENDCOLOR}"
              echo "$sockets" | sed 's/^/      /'
              sockets_found=$((sockets_found + $(echo "$sockets" | wc -l)))
            fi
          fi
        done
        
        if [ -z "$active_connections" ] && [ $sockets_found -eq 0 ]; then
          echo -e "${BLUE}    No SSH connections or sockets found for $ip${ENDCOLOR}"
        fi
      done
      
      echo -e "${BLUE}Total connections to close: $connections_found${ENDCOLOR}"
      echo -e "${BLUE}Total sockets to remove: $sockets_found${ENDCOLOR}"
      echo -e "${BLUE}Run without --dry-run to actually clear connections.${ENDCOLOR}"
      exit 0
    fi
    
    # Actually clear SSH connections and sockets
    cleared_connections=0
    cleared_sockets=0
    
    for ip in "${vm_ips_to_clear[@]}"; do
      echo -e "${BLUE}  Clearing SSH connections for $ip...${ENDCOLOR}"
      
      # Kill active SSH connections
      ssh_pids=$(ps aux | grep -E "ssh.*$ip" | grep -v grep | grep -v "clear-ssh-maps" | awk '{print $2}' || true)
      if [ -n "$ssh_pids" ]; then
        for pid in $ssh_pids; do
          if kill "$pid" 2>/dev/null; then
            echo -e "${GREEN}    Closed SSH connection (PID: $pid)${ENDCOLOR}"
            cleared_connections=$((cleared_connections + 1))
          fi
        done
      fi
      
      # Remove SSH control sockets
      socket_locations=(
        "$HOME/.ssh/sockets"
        "$HOME/.ssh/connections"
        "$HOME/.ssh/master"
        "/tmp"
      )
      
      for socket_dir in "${socket_locations[@]}"; do
        if [ -d "$socket_dir" ]; then
          sockets=$(find "$socket_dir" -name "*$ip*" -type s 2>/dev/null || true)
          if [ -n "$sockets" ]; then
            while IFS= read -r socket; do
              if [ -n "$socket" ] && rm -f "$socket" 2>/dev/null; then
                echo -e "${GREEN}    Removed SSH socket: $(basename "$socket")${ENDCOLOR}"
                cleared_sockets=$((cleared_sockets + 1))
              fi
            done <<< "$sockets"
          fi
        fi
      done
    done
    
    # Also check for SSH master connections that might use different naming
    echo -e "${BLUE}Checking for SSH master connections...${ENDCOLOR}"
    for ip in "${vm_ips_to_clear[@]}"; do
      # Try to close SSH master connections using ssh -O exit
      if ssh -O check "$ip" 2>/dev/null; then
        if ssh -O exit "$ip" 2>/dev/null; then
          echo -e "${GREEN}  Closed SSH master connection to $ip${ENDCOLOR}"
          cleared_connections=$((cleared_connections + 1))
        fi
      fi
    done
    
    echo -e "${GREEN}SSH connection cleanup completed:${ENDCOLOR}"
    echo -e "${GREEN}  - Closed $cleared_connections active connections${ENDCOLOR}"
    echo -e "${GREEN}  - Removed $cleared_sockets control sockets${ENDCOLOR}"
    
    if [ $cleared_connections -eq 0 ] && [ $cleared_sockets -eq 0 ]; then
      echo -e "${YELLOW}No SSH connections or sockets were found to clear.${ENDCOLOR}"
    fi
    ;;

  add-nodes)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc add-nodes [--target-hosts <hosts>] [--node-type <worker|control>]"
      echo ""
      echo "Add new nodes to the Kubernetes cluster."
      echo ""
      echo "Options:"
      echo "  --target-hosts <hosts>  Specify target hosts (default: new_workers)"
      echo "  --node-type <type>      Node type: worker or control (default: worker)"
      echo ""
      echo "Note: Ensure new nodes are added to your Terraform configuration first."
      exit 0
    fi

    # Parse command line arguments
    target_hosts="new_workers"
    node_type="worker"
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --target-hosts)
          target_hosts="$2"
          shift 2
          ;;
        --node-type)
          node_type="$2"
          shift 2
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          exit 1
          ;;
      esac
    done

    echo -e "${BLUE}Adding $node_type nodes to the cluster...${ENDCOLOR}"
    run_ansible_playbook "pb_add_nodes.yml" "$target_hosts" "-e node_type=$node_type"
    ;;

  drain-node)
    if [[ -z "$1" || "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc drain-node <node_name> [--force] [--delete-emptydir-data]" # Changed from ccr
      exit 1
    fi
    node_name="$1"
    shift
    extra_cli_opts="$*" # Pass through any remaining options like --force
    run_ansible_playbook "pb_drain_node.yml" "all" "-e node_to_drain=$node_name -e drain_options='''$extra_cli_opts'''"
    ;;

  delete-node)
    if [[ -z "$1" || "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc delete-node <node_name> [--reset-node]" # Changed from ccr
      exit 1
    fi
    node_name="$1"
    reset_flag="${2:---no-reset-node}" # default to not resetting
    run_ansible_playbook "pb_delete_node.yml" "all" "-e node_to_delete=$node_name -e reset_after_delete=$reset_flag"
    ;;

  upgrade-node)
    if [[ -z "$1" || "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc upgrade-node <node_name> [--target-version <version>] [--skip-drain]"
      echo ""
      echo "Upgrade Kubernetes on a specific node."
      echo ""
      echo "Options:"
      echo "  --target-version <version>  Target Kubernetes version (default: from environment)"
      echo "  --skip-drain               Skip draining the node before upgrade"
      echo ""
      echo "The upgrade process will:"
      echo "  1. Drain the node (unless --skip-drain is specified)"
      echo "  2. Upgrade Kubernetes packages"
      echo "  3. Restart kubelet service"
      echo "  4. Uncordon the node"
      exit 1
    fi
    
    node_name="$1"
    shift
    
    # Parse remaining arguments
    target_version=""
    skip_drain="false"
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --target-version)
          target_version="$2"
          shift 2
          ;;
        --skip-drain)
          skip_drain="true"
          shift
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          exit 1
          ;;
      esac
    done

    extra_vars="-e target_node=$node_name -e skip_drain=$skip_drain"
    if [ -n "$target_version" ]; then
      extra_vars="$extra_vars -e target_k8s_version=$target_version"
    fi

    echo -e "${BLUE}Upgrading Kubernetes on node: $node_name${ENDCOLOR}"
    run_ansible_playbook "pb_upgrade_node.yml" "$node_name" "$extra_vars"
    ;;

  reset-node)
    if [[ -z "$1" || "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc reset-node <node_name_or_ip>" # Changed from ccr
      exit 1
    fi
    node_name="$1"
    run_ansible_playbook "pb_reset_node.yml" "$node_name" "" # Target specific node
    ;;

  reset-all-nodes)
    read -r -p "Are you sure you want to reset Kubernetes on ALL nodes in context '$(get_current_cluster_context)'? [y/N] " response
    if [[ "$response" =~ ^([yY][eE][sS]|[yY])$ ]]; then
      run_ansible_playbook "pb_reset_all_nodes.yml" "all" ""
    else
      echo "Operation cancelled."
    fi
    ;;

  upgrade-addons)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc upgrade-addons [--addon <name>] [--version <version>]"
      echo ""
      echo "Install or upgrade cluster addons. Shows interactive menu if no --addon specified."
      echo ""
      echo "Options:"
      echo "  --addon <name>     Force specific addon to install/upgrade (skips menu)"
      echo "  --version <version> Target version for the addon (default: from environment variables)"
      echo ""
      echo "Available addons:"
      echo "  - calico: Calico CNI networking"
      echo "  - metallb: MetalLB load balancer"
      echo "  - metrics-server: Kubernetes Metrics Server"
      echo "  - coredns: CoreDNS DNS server"
      echo "  - cert-manager: Certificate manager for Kubernetes"
      echo "  - kubelet-serving-cert-approver: Automatic approval of kubelet serving certificates"
      echo "  - argocd: ArgoCD GitOps continuous delivery"
      echo "  - ingress-nginx: NGINX Ingress Controller"
      echo "  - all: Install/upgrade all addons"
      echo ""
      echo "Examples:"
      echo "  cpc upgrade-addons                    # Show interactive menu"
      echo "  cpc upgrade-addons --addon all        # Install all addons directly"
      echo "  cpc upgrade-addons --addon calico     # Install only Calico"
      exit 0
    fi

    # Parse command line arguments
    addon_name=""
    addon_version=""
    force_addon=""
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --addon)
          force_addon="$2"
          shift 2
          ;;
        --version)
          addon_version="$2"
          shift 2
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          exit 1
          ;;
      esac
    done

    # If no addon specified via --addon, show interactive menu
    if [ -z "$force_addon" ]; then
      echo -e "${BLUE}Select addon to install/upgrade:${ENDCOLOR}"
      echo ""
      echo "  1) all                                 - Install/upgrade all addons"
      echo "  2) calico                              - Calico CNI networking"
      echo "  3) metallb                             - MetalLB load balancer"
      echo "  4) metrics-server                      - Kubernetes Metrics Server" 
      echo "  5) coredns                             - CoreDNS DNS server"
      echo "  6) cert-manager                        - Certificate manager"
      echo "  7) kubelet-serving-cert-approver       - Kubelet cert approver"
      echo "  8) argocd                              - ArgoCD GitOps"
      echo "  9) ingress-nginx                       - NGINX Ingress Controller"
      echo ""
      read -r -p "Enter your choice [1-9]: " choice
      
      case $choice in
        1) addon_name="all" ;;
        2) addon_name="calico" ;;
        3) addon_name="metallb" ;;
        4) addon_name="metrics-server" ;;
        5) addon_name="coredns" ;;
        6) addon_name="cert-manager" ;;
        7) addon_name="kubelet-serving-cert-approver" ;;
        8) addon_name="argocd" ;;
        9) addon_name="ingress-nginx" ;;
        *) 
          echo -e "${RED}Invalid choice: $choice${ENDCOLOR}" >&2
          exit 1
          ;;
      esac
    else
      addon_name="$force_addon"
    fi

    # Validate addon name
    case "$addon_name" in
      calico|metallb|metrics-server|coredns|cert-manager|kubelet-serving-cert-approver|argocd|ingress-nginx|all)
        ;;
      *)
        echo -e "${RED}Invalid addon name: $addon_name${ENDCOLOR}" >&2
        echo -e "${RED}Valid options: calico, metallb, metrics-server, coredns, cert-manager, kubelet-serving-cert-approver, argocd, ingress-nginx, all${ENDCOLOR}" >&2
        exit 1
        ;;
    esac

    extra_vars="-e addon_name=$addon_name"
    if [ -n "$addon_version" ]; then
      extra_vars="$extra_vars -e addon_version=$addon_version"
    fi

    echo -e "${BLUE}Installing/upgrading cluster addon(s): $addon_name${ENDCOLOR}"
    run_ansible_playbook "pb_upgrade_addons_extended.yml" "control_plane" "$extra_vars"
    ;;

  upgrade-k8s)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc upgrade-k8s [--target-version <version>] [--skip-etcd-backup]"
      echo ""
      echo "Upgrade Kubernetes control plane components."
      echo ""
      echo "Options:"
      echo "  --target-version <version>  Target Kubernetes version (default: from environment)"
      echo "  --skip-etcd-backup         Skip etcd backup before upgrade"
      echo ""
      echo "The upgrade process will:"
      echo "  1. Backup etcd (unless --skip-etcd-backup is specified)"
      echo "  2. Upgrade control plane components on each control plane node"
      echo "  3. Verify cluster health after upgrade"
      echo ""
      echo "Warning: This will upgrade the control plane. Ensure you have backups!"
      exit 0
    fi

    # Parse command line arguments
    target_version=""
    skip_etcd_backup="false"
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --target-version)
          target_version="$2"
          shift 2
          ;;
        --skip-etcd-backup)
          skip_etcd_backup="true"
          shift
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          exit 1
          ;;
      esac
    done

    # Confirmation prompt
    current_ctx=$(get_current_cluster_context)
    echo -e "${YELLOW}Warning: This will upgrade the Kubernetes control plane for context '$current_ctx'.${ENDCOLOR}"
    read -r -p "Are you sure you want to continue? [y/N] " response
    if [[ ! "$response" =~ ^([yY][eE][sS]|[yY])$ ]]; then
      echo "Operation cancelled."
      exit 0
    fi

    extra_vars="-e skip_etcd_backup=$skip_etcd_backup"
    if [ -n "$target_version" ]; then
      extra_vars="$extra_vars -e target_k8s_version=$target_version"
    fi

    echo -e "${BLUE}Upgrading Kubernetes control plane...${ENDCOLOR}"
    run_ansible_playbook "pb_upgrade_k8s_control_plane.yml" "control_plane" "$extra_vars"
    ;;

  vmctl)
    echo -e "${BLUE}VM control (start, stop, create, delete) is primarily managed by Tofu in this project.${ENDCOLOR}" # Changed from Terraform
    echo -e "${BLUE}Please use 'tofu apply', 'tofu destroy', or modify your .tfvars and re-apply.${ENDCOLOR}" # Changed from terraform
    echo -e "${BLUE}Example: To stop a VM, you might comment it out in Tofu and apply, or use Proxmox UI/API directly.${ENDCOLOR}" # Changed from Terraform
    # Placeholder for future direct VM interactions if needed via Proxmox API etc.
    # run_ansible_playbook "pb_vm_control.yml" "localhost" "-e vm_name=$1 -e action=$2"
    ;;

  run-command)
    if [[ $# -lt 2 || "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc run-command <target_hosts_or_group> "<shell_command_to_run>"" # Changed from ccr
      echo "Example: cpc run-command control_plane_nodes "hostname -f"" # Changed from ccr
      echo "Example: cpc run-command all "sudo apt update"" # Changed from ccr
      exit 1
    fi
    target="$1"
    shell_cmd="$2"
    run_ansible_playbook "pb_run_command.yml" "$target" "-e command_to_run='''$shell_cmd'''"
    ;;

  update-pihole)
    if [[ "$1" == "-h" || "$1" == "--help" ]] || [[ -z "$1" ]]; then # Added check for missing argument
      echo "Usage: cpc update-pihole <action>" # Changed from ccr
      echo "Manages Pi-hole DNS records with VM FQDNs and IPs from the current Tofu workspace outputs." # Changed
      echo "Actions: 'add', 'unregister-dns'." # Added
      echo "Requires 'sops' and 'curl' to be installed, and secrets.sops.yaml to be configured."
      exit 0
    fi
    action="$1" # Removed local
    if [[ "$action" != "add" && "$action" != "unregister-dns" ]]; then
      echo -e "${RED}Error: Invalid action '$action' for update-pihole. Must be 'add' or 'unregister-dns'.${ENDCOLOR}" >&2
      exit 1
    fi

    echo -e "${BLUE}Updating Pi-hole DNS records (action: $action)...${ENDCOLOR}" # Changed
    # Assuming the script is in REPO_PATH/scripts/
    "$REPO_PATH/scripts/add_pihole_dns.py" --action "$action" --secrets-file "$REPO_PATH/terraform/secrets.sops.yaml" --tf-dir "$REPO_PATH/terraform" # Added --action
    if [ $? -ne 0 ]; then
      echo -e "${RED}Error updating Pi-hole DNS records.${ENDCOLOR}" >&2
      exit 1
    fi
    echo -e "${GREEN}Pi-hole DNS update script finished.${ENDCOLOR}"
    ;;

  deploy)
    if [[ "$1" == "-h" || "$1" == "--help" ]] || [[ $# -eq 0 ]]; then
      echo "Usage: cpc deploy <tofu_command> [additional_tofu_options]"
      echo ""
      echo "Runs the specified 'tofu' command (e.g., plan, apply, validate, output, destroy) in the"
      echo "Terraform directory for the current cpc context."
      echo ""
      echo "Key features:"
      echo "  - Automatically changes to the '$REPO_PATH/terraform' directory."
      echo "  - Ensures the Tofu workspace matches the current cpc context (e.g., 'debian', 'ubuntu')."
      echo "  - For 'tofu plan', 'apply', 'destroy', 'import', and 'console' commands, automatically includes"
      echo "    the context-specific variables file (e.g., 'environments/\$CONTEXT.tfvars') if it exists."
      echo ""
      echo "Examples:"
      echo "  cpc deploy plan                                 # Plan changes for the current context"
      echo "  cpc deploy apply -auto-approve                  # Apply changes with auto-approval"
      echo "  cpc deploy output k8s_node_ips             # Get a specific output"
      echo "  cpc deploy validate                             # Validate the configuration"
      echo "  cpc deploy destroy -target=proxmox_vm_kvm.vm[0] # Destroy a specific resource"
      echo "  cpc deploy workspace list                       # List Tofu workspaces"
      exit 0
    fi

    current_ctx=$(get_current_cluster_context)
    tf_dir="$REPO_PATH/terraform"
    tfvars_file="$tf_dir/environments/${current_ctx}.tfvars"

    echo -e "${BLUE}Preparing to run 'tofu $*' for context '$current_ctx' in $tf_dir...${ENDCOLOR}"

    pushd "$tf_dir" > /dev/null || { echo -e "${RED}Failed to change to directory $tf_dir${ENDCOLOR}"; exit 1; }
    selected_workspace=$(tofu workspace show)
    if [ "$selected_workspace" != "$current_ctx" ]; then
        echo -e "${YELLOW}Warning: Current Tofu workspace ('$selected_workspace') does not match cpc context ('$current_ctx').${ENDCOLOR}"
        echo -e "${YELLOW}Attempting to select workspace '$current_ctx'...${ENDCOLOR}"
        tofu workspace select "$current_ctx"
        if [ $? -ne 0 ]; then
            echo -e "${RED}Error selecting Tofu workspace '$current_ctx'. Please check your Tofu setup.${ENDCOLOR}" >&2
            popd > /dev/null
            exit 1
        fi
    fi

    tofu_subcommand="$1"
    shift # Remove subcommand, rest are its arguments

    final_tofu_cmd_array=(tofu "$tofu_subcommand")
    
    # Generate node hostname configurations for Proxmox if applying or planning
    if [ "$tofu_subcommand" = "apply" ] || [ "$tofu_subcommand" = "plan" ]; then
      echo -e "${BLUE}Generating node hostname configurations...${ENDCOLOR}"
      if [ -x "$REPO_PATH/scripts/generate_node_hostnames.sh" ]; then
        pushd "$REPO_PATH/scripts" > /dev/null
        ./generate_node_hostnames.sh
        HOSTNAME_SCRIPT_STATUS=$?
        popd > /dev/null
        
        if [ $HOSTNAME_SCRIPT_STATUS -ne 0 ]; then
          echo -e "${YELLOW}Warning: Hostname generation script returned non-zero status. Some VMs may have incorrect hostnames.${ENDCOLOR}"
        else
          echo -e "${GREEN}Hostname configurations generated successfully.${ENDCOLOR}"
        fi
      else
        echo -e "${YELLOW}Warning: Hostname generation script not found or not executable. Some VMs may have incorrect hostnames.${ENDCOLOR}"
      fi
    fi

    # Check if the subcommand is one that accepts -var-file
    case "$tofu_subcommand" in
      apply|plan|destroy|import|console)
        if [ -f "$tfvars_file" ]; then
          final_tofu_cmd_array+=("-var-file=$tfvars_file")
          echo -e "${BLUE}Using tfvars file: $tfvars_file${ENDCOLOR}"
        else
          echo -e "${YELLOW}Warning: No specific tfvars file found for context '$current_ctx' at $tfvars_file. Using defaults if applicable.${ENDCOLOR}"
        fi
        ;;
    esac

    # Append remaining user-provided arguments
    if [[ $# -gt 0 ]]; then
      final_tofu_cmd_array+=("$@")
    fi

    echo -e "${BLUE}Executing: ${final_tofu_cmd_array[*]}${ENDCOLOR}"
    "${final_tofu_cmd_array[@]}"
    cmd_exit_code=$?

    popd > /dev/null || exit 1

    if [ $cmd_exit_code -ne 0 ]; then
      echo -e "${RED}Error: '${final_tofu_cmd_array[*]}' failed with exit code $cmd_exit_code.${ENDCOLOR}" >&2
      exit 1
    fi
    echo -e "${GREEN}'${final_tofu_cmd_array[*]}' completed successfully for context '$current_ctx'.${ENDCOLOR}"
    ;;

  generate-hostnames)
    check_secrets_loaded
    echo -e "${GREEN}Generating node hostname snippets...${ENDCOLOR}"
    "$REPO_PATH/scripts/generate_node_hostnames.sh"
    ;;

  gen_hostnames)
    echo "Generating node hostname configurations..."
    if [[ -f "$REPO_PATH/scripts/generate_node_hostnames.sh" ]]; then
      pushd "$REPO_PATH/scripts" > /dev/null
      ./generate_node_hostnames.sh
      HOSTNAME_SCRIPT_STATUS=$?
      popd > /dev/null
      
      if [ $HOSTNAME_SCRIPT_STATUS -ne 0 ]; then
        echo -e "${RED}Error: Hostname generation failed with status $HOSTNAME_SCRIPT_STATUS${ENDCOLOR}"
        exit 1
      else
        echo -e "${GREEN}Hostname configurations generated successfully.${ENDCOLOR}"
      fi
    else 
      echo -e "${RED}Hostname generation script not found at $REPO_PATH/scripts/generate_node_hostnames.sh${ENDCOLOR}"
      exit 1
    fi
    ;;

  start-vms)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc start-vms"
      echo "Starts all VMs in the current cpc context by running 'tofu apply' with vm_started=true."
      exit 0
    fi
    current_ctx=$(get_current_cluster_context)
    echo -e "${BLUE}Starting VMs for context '$current_ctx'...${ENDCOLOR}"
    # Call the deploy command internally, passing the appropriate tofu apply options
    # Ensure that the deploy command is in the PATH or use its full path if cpc is not installed globally
    # Assuming cpc is in PATH for simplicity here. If not, use: "$0" deploy ...
    "$0" deploy apply -var="vm_started=true" -auto-approve
    if [ $? -ne 0 ]; then
      echo -e "${RED}Error starting VMs for context '$current_ctx'.${ENDCOLOR}" >&2
      exit 1
    fi
    echo -e "${GREEN}VMs in context '$current_ctx' should now be starting.${ENDCOLOR}"
    ;;

  stop-vms)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc stop-vms"
      echo "Stops all VMs in the current cpc context by running 'tofu apply' with vm_started=false."
      exit 0
    fi
    current_ctx=$(get_current_cluster_context)
    echo -e "${BLUE}Stopping VMs for context '$current_ctx'...${ENDCOLOR}"
    read -r -p "Are you sure you want to stop all VMs in context '$current_ctx'? [y/N] " response
    if [[ ! "$response" =~ ^([yY][eE][sS]|[yY])$ ]]; then
      echo "Operation cancelled."
      exit 0
    fi
    "$0" deploy apply -var="vm_started=false" -auto-approve
    if [ $? -ne 0 ]; then
      echo -e "${RED}Error stopping VMs for context '$current_ctx'.${ENDCOLOR}" >&2
      exit 1
    fi
    echo -e "${GREEN}VMs in context '$current_ctx' should now be stopping.${ENDCOLOR}"
    ;;

  "" | "-h" | "--help" | "help")
    display_usage
    ;;

  scripts/*)
    # Handle running scripts directly: ./cpc scripts/script_name.sh
    script_path="$REPO_PATH/$COMMAND"
    if [[ -f "$script_path" && -x "$script_path" ]]; then
      echo -e "${BLUE}Running script: $script_path${ENDCOLOR}"
      # Pass all remaining arguments to the script
      "$script_path" "$@"
    elif [[ -f "$script_path" ]]; then
      echo -e "${RED}Error: Script $script_path exists but is not executable.${ENDCOLOR}" >&2
      echo -e "${BLUE}Try: chmod +x $script_path${ENDCOLOR}" >&2
      exit 1
    else
      echo -e "${RED}Error: Script not found at $script_path${ENDCOLOR}" >&2
      exit 1
    fi
    ;;

  *)
    echo -e "${RED}Unknown command: $COMMAND${ENDCOLOR}" >&2
    display_usage
    exit 1
    ;;
esac

exit 0
