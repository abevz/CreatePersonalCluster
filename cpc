#!/bin/bash

# =============================================================================
# CPC (Cluster Provision & Configure) - Main Script
# =============================================================================
# Enhanced with modular architecture for better maintainability

# Color definitions (kept for backward compatibility)
export GREEN='\033[32m'
export RED='\033[0;31m'
export YELLOW='\033[0;33m'
export BLUE='\033[1;34m'
export ENDCOLOR='\033[0m'

# --- Load Modular Architecture ---
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Load configuration first
if [ -f "$SCRIPT_DIR/config.conf" ]; then
    source "$SCRIPT_DIR/config.conf"
fi

# Load libraries
for lib in "$SCRIPT_DIR/lib"/*.sh; do
    [ -f "$lib" ] && source "$lib"
done

# Load modules  
for module in "$SCRIPT_DIR/modules"/*.sh; do
    [ -f "$module" ] && source "$module"
done

# Set REPO_PATH for modules
export REPO_PATH="$SCRIPT_DIR"

# Configuration (legacy, will be moved to config.conf)
CONFIG_DIR="$HOME/.config/cpc" # Updated for CreatePersonalCluster project
REPO_PATH_FILE="$CONFIG_DIR/repo_path"
CLUSTER_CONTEXT_FILE="$CONFIG_DIR/current_cluster_context"
CPC_ENV_FILE="cpc.env" # Expect this in the repo root, Changed from CCR_ENV_FILE and ccr.env

# --- Helper Functions ---
check_required_commands() {
  for cmd in "$@"; do
    if ! command -v "$cmd" &> /dev/null; then
      echo -e "${RED}Error: '$cmd' is required but not installed. Please install it before proceeding.${ENDCOLOR}" >&2
      exit 1
    fi
  done
}
export -f check_required_commands

get_repo_path() {
  if [ -f "$REPO_PATH_FILE" ]; then
    cat "$REPO_PATH_FILE"
  else
    echo -e "${RED}Repository path not set. Run 'cpc setup-cpc' to set this value.${ENDCOLOR}" >&2 # Changed from ccr setup-ccr
    exit 1
  fi
}
export -f get_repo_path

get_current_cluster_context() {
  if [ -f "$CLUSTER_CONTEXT_FILE" ]; then
    cat "$CLUSTER_CONTEXT_FILE"
  else
    echo -e "${RED}Error: No cpc context set.${ENDCOLOR}" >&2
    echo -e "${BLUE}The cpc context determines the Tofu workspace and associated configuration (e.g., OS type).${ENDCOLOR}" >&2
    echo -e "${BLUE}Please set a context using 'cpc ctx <workspace_name>'.${ENDCOLOR}" >&2
    
    # Attempt to get repo_path to list workspaces.
    # This relies on REPO_PATH_FILE being set by 'cpc setup-cpc'.
    if [ -f "$REPO_PATH_FILE" ]; then
      local repo_p_for_listing
      repo_p_for_listing=$(cat "$REPO_PATH_FILE")
      if [ -d "$repo_p_for_listing/terraform" ]; then
        echo -e "${BLUE}Available Tofu workspaces in '$repo_p_for_listing/terraform' (use one of these for <workspace_name>):${ENDCOLOR}" >&2
        # Ensure tofu command is available for listing or provide a message
        if command -v tofu &> /dev/null; then
          (cd "$repo_p_for_listing/terraform" && tofu workspace list | sed 's/^*/  /') >&2
        else
          echo -e "${YELLOW}  'tofu' command not found. Cannot list workspaces. Please ensure OpenTofu is installed and in your PATH.${ENDCOLOR}" >&2
        fi
      else
        echo -e "${YELLOW}Warning: Cannot list Tofu workspaces. Terraform directory not found at '$repo_p_for_listing/terraform'.${ENDCOLOR}" >&2
      fi
    else
      echo -e "${YELLOW}Warning: Cannot list Tofu workspaces. Repository path not set. Run 'cpc setup-cpc'.${ENDCOLOR}" >&2
    fi
    echo -e "${BLUE}Typically, the context/workspace should be one of: debian, ubuntu, rocky.${ENDCOLOR}" >&2
    exit 1
  fi
}
export -f get_current_cluster_context

# Check if secrets are already loaded
check_secrets_loaded() {
  if [ -z "$PROXMOX_HOST" ] || [ -z "$PROXMOX_USERNAME" ] || [ -z "$VM_USERNAME" ]; then
    echo -e "${RED}Error: Secrets not loaded. This command requires SOPS secrets to be loaded.${ENDCOLOR}" >&2
    echo -e "${BLUE}Please run 'cpc load_secrets' first or ensure cpc.env is properly configured.${ENDCOLOR}" >&2
    exit 1
  fi
}
export -f check_secrets_loaded

# Load sensitive data from secrets.sops.yaml using SOPS
load_secrets() {
  local repo_root
  repo_root=$(get_repo_path)
  local secrets_file="$repo_root/terraform/secrets.sops.yaml"
  
  if [ ! -f "$secrets_file" ]; then
    echo -e "${RED}Error: secrets.sops.yaml not found at $secrets_file${ENDCOLOR}" >&2
    exit 1
  fi
  
  # Check if sops is installed
  if ! command -v sops &> /dev/null; then
    echo -e "${RED}Error: 'sops' is required but not installed. Please install it before proceeding.${ENDCOLOR}" >&2
    exit 1
  fi
  
  # Check if jq is installed
  if ! command -v jq &> /dev/null; then
    echo -e "${RED}Error: 'jq' is required but not installed. Please install it before proceeding.${ENDCOLOR}" >&2
    exit 1
  fi
  
  echo -e "${BLUE}Loading secrets from secrets.sops.yaml...${ENDCOLOR}"
  
  # Export sensitive variables from SOPS
  export PROXMOX_HOST
  export PROXMOX_USERNAME  
  export PROXMOX_PASSWORD
  export VM_USERNAME
  export VM_PASSWORD
  export VM_SSH_KEY
  export AWS_ACCESS_KEY_ID
  export AWS_SECRET_ACCESS_KEY
  export AWS_DEFAULT_REGION
  
  # Load secrets using sops, convert to JSON, then parse with jq
  local secrets_json
  secrets_json=$(sops -d "$secrets_file" 2>/dev/null | python3 -c "import sys, yaml, json; json.dump(yaml.safe_load(sys.stdin), sys.stdout)")
  
  if [ $? -ne 0 ]; then
    echo -e "${RED}Error: Failed to decrypt secrets.sops.yaml. Check your SOPS configuration.${ENDCOLOR}" >&2
    exit 1
  fi
  
  # Parse secrets from JSON
  PROXMOX_HOST=$(echo "$secrets_json" | jq -r '.virtual_environment_endpoint' | sed 's|https://||' | sed 's|:8006/api2/json||')
  PROXMOX_USERNAME=$(echo "$secrets_json" | jq -r '.proxmox_username')
  PROXMOX_PASSWORD=$(echo "$secrets_json" | jq -r '.virtual_environment_password')
  VM_USERNAME=$(echo "$secrets_json" | jq -r '.vm_username')
  VM_PASSWORD=$(echo "$secrets_json" | jq -r '.vm_password')
  VM_SSH_KEY=$(echo "$secrets_json" | jq -r '.vm_ssh_keys[0]')
  
  # Parse MinIO/S3 credentials for Terraform backend
  AWS_ACCESS_KEY_ID=$(echo "$secrets_json" | jq -r '.minio_access_key')
  AWS_SECRET_ACCESS_KEY=$(echo "$secrets_json" | jq -r '.minio_secret_key')
  AWS_DEFAULT_REGION="us-east-1"  # Set default region for MinIO
  
  # Verify that all required secrets were loaded
  if [ -z "$PROXMOX_HOST" ] || [ -z "$PROXMOX_USERNAME" ] || [ -z "$PROXMOX_PASSWORD" ] || [ -z "$VM_USERNAME" ] || [ -z "$VM_PASSWORD" ] || [ -z "$VM_SSH_KEY" ] || [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_SECRET_ACCESS_KEY" ]; then
    echo -e "${RED}Error: Failed to load one or more required secrets from secrets.sops.yaml${ENDCOLOR}" >&2
    echo -e "${BLUE}Required secrets: PROXMOX_HOST, PROXMOX_USERNAME, PROXMOX_PASSWORD, VM_USERNAME, VM_PASSWORD, VM_SSH_KEY, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY${ENDCOLOR}" >&2
    exit 1
  fi
  
  echo -e "${BLUE}Successfully loaded secrets (PROXMOX_HOST: $PROXMOX_HOST, VM_USERNAME: $VM_USERNAME)${ENDCOLOR}"
}
export -f load_secrets

# Source environment variables if cpc.env exists and set workspace-specific variables
load_env_vars() {
  local repo_root
  repo_root=$(get_repo_path)
  
  # Load secrets first
  load_secrets
  
  if [ -f "$repo_root/$CPC_ENV_FILE" ]; then # Changed from CCR_ENV_FILE
    # shellcheck source=cpc.env
    set -a # Automatically export all variables
    source "$repo_root/$CPC_ENV_FILE" # Changed from CCR_ENV_FILE
    set +a # Stop automatically exporting
    echo -e "${BLUE}Loaded environment variables from $CPC_ENV_FILE${ENDCOLOR}" # Changed from CCR_ENV_FILE
    
    # Export static IP configuration variables to Terraform
    if [ -n "$NETWORK_CIDR" ]; then
      export TF_VAR_network_cidr="$NETWORK_CIDR"
    fi
    if [ -n "$STATIC_IP_START" ]; then
      export TF_VAR_static_ip_start="$STATIC_IP_START"
    fi
    if [ -n "$WORKSPACE_IP_BLOCK_SIZE" ]; then
      export TF_VAR_workspace_ip_block_size="$WORKSPACE_IP_BLOCK_SIZE"
    fi
    if [ -n "$STATIC_IP_BASE" ]; then
      export TF_VAR_static_ip_base="$STATIC_IP_BASE"
    fi
    if [ -n "$STATIC_IP_GATEWAY" ]; then
      export TF_VAR_static_ip_gateway="$STATIC_IP_GATEWAY"
    fi
    
    # Set workspace-specific template variables based on current context
    # This function should only be called after REPO_PATH is set
    if [ -f "$CLUSTER_CONTEXT_FILE" ]; then
      local current_workspace
      current_workspace=$(cat "$CLUSTER_CONTEXT_FILE")
      set_workspace_template_vars "$current_workspace"
    fi
  else
    echo -e "${YELLOW}Warning: $CPC_ENV_FILE not found in repository root. Some default versions might be used by playbooks.${ENDCOLOR}" # Changed from CCR_ENV_FILE
  fi
}
export -f load_env_vars

# Set workspace-specific template variables based on current workspace
set_workspace_template_vars() {
  local workspace="$1"
  
  if [ -z "$workspace" ]; then
    echo -e "${YELLOW}Warning: No workspace specified for setting template variables${ENDCOLOR}"
    return 1
  fi
  
  # Path to the environment file
  local env_file="$REPO_PATH/envs/$workspace.env"
  
  # Check if the environment file exists
  if [[ ! -f "$env_file" ]]; then
    echo -e "${YELLOW}Warning: Environment file for workspace '$workspace' not found.${ENDCOLOR}"
    # Dynamically list available workspaces
    echo -ne "${BLUE}Available workspaces: ${ENDCOLOR}"
    ls -1 "$REPO_PATH/envs/" | grep -E '\.env$' | sed 's/\.env$//' | tr '\n' ', ' | sed 's/,$/\n/'
    return 1
  fi
  
  # Load variables from environment file
  source "$env_file"
  
  # Set default values for any variables that might be missing
  : "${TEMPLATE_VM_ID:=900}"
  : "${TEMPLATE_VM_NAME:=tpl-default-k8s}"
  : "${IMAGE_NAME:=default-image.img}"
  : "${IMAGE_LINK:=https://example.com/default-image.img}"
  : "${KUBERNETES_SHORT_VERSION:=1.29}"
  : "${KUBERNETES_MEDIUM_VERSION:=v1.29}"
  : "${KUBERNETES_VERSION:=$KUBERNETES_MEDIUM_VERSION}"
  : "${KUBERNETES_LONG_VERSION:=1.29.0}"
  : "${CNI_PLUGINS_VERSION:=v1.4.0}"
  : "${CALICO_VERSION:=v3.26.0}"
  : "${METALLB_VERSION:=v0.14.3}"
  : "${COREDNS_VERSION:=v1.10.1}"
  : "${METRICS_SERVER_VERSION:=v0.6.4}"
  : "${ETCD_VERSION:=v3.5.10}"
  : "${KUBELET_SERVING_CERT_APPROVER_VERSION:=v0.1.8}"
  : "${LOCAL_PATH_PROVISIONER_VERSION:=v0.0.24}"
  : "${CERT_MANAGER_VERSION:=v1.15.0}"
  : "${ARGOCD_VERSION:=v2.12.0}"
  : "${INGRESS_NGINX_VERSION:=v1.11.0}"
  
  echo -e "${BLUE}Set template variables for workspace '$workspace':${ENDCOLOR}"
  echo -e "${BLUE}  TEMPLATE_VM_ID: $TEMPLATE_VM_ID${ENDCOLOR}"
  echo -e "${BLUE}  TEMPLATE_VM_NAME: $TEMPLATE_VM_NAME${ENDCOLOR}"
  echo -e "${BLUE}  IMAGE_NAME: $IMAGE_NAME${ENDCOLOR}"
  echo -e "${BLUE}  KUBERNETES_VERSION: $KUBERNETES_MEDIUM_VERSION${ENDCOLOR}"
  echo -e "${BLUE}  CALICO_VERSION: $CALICO_VERSION${ENDCOLOR}"
  echo -e "${BLUE}  METALLB_VERSION: $METALLB_VERSION${ENDCOLOR}"
  echo -e "${BLUE}  COREDNS_VERSION: $COREDNS_VERSION${ENDCOLOR}"
  echo -e "${BLUE}  ETCD_VERSION: $ETCD_VERSION${ENDCOLOR}"
}
export -f set_workspace_template_vars

# TODO: Move to modules/20_ansible.sh - run_ansible_playbook function has been modularized as ansible_run_playbook()

display_usage() {
  echo "Usage: cpc <command> [options]"
  echo ""
  echo "Commands:"
  echo "  setup-cpc                      Initial setup for cpc command."
  echo "  ctx [<cluster_name>]           Get or set the current cluster context (Tofu workspace)."
  echo "  clone-workspace <src> <dst>    Clone a workspace environment to create a new one."
  echo "  delete-workspace <n>           Delete a workspace environment."
  echo "  template                       Creates a VM template for Kubernetes"
  echo "  run-playbook <playbook>        Run any Ansible playbook from ansible/playbooks/"
  echo "  run-command <target> \"<cmd>\"   Run a shell command on target host(s) or group."
  echo "  clear-ssh-hosts                Clear VM IP addresses from ~/.ssh/known_hosts"
  echo "  clear-ssh-maps                 Clear SSH control sockets and connections for VMs"
  echo "  load_secrets                   Load and display secrets from SOPS configuration"
  echo "  dns-pihole <action>            Manage Pi-hole DNS records. Actions: list, add, unregister-dns, interactive-add, interactive-unregister."
  echo "  generate-hostnames             Generate hostname configurations for VMs in Proxmox"
  echo "  scripts/<script_name>          Run any script from the scripts directory"
  echo "  deploy <tofu_cmd> [opts]       Run any 'tofu' command (e.g., plan, apply, output) in context."
  echo "  cluster-info                   Show simplified cluster information (VM_ID, hostname, IP)."
  echo ""
  echo "VM Management:"
  echo "  add-vm                         Interactively add a new VM (worker or control plane)."
  echo "  remove-vm                      Interactively remove a VM and update configuration."
  echo "  start-vms                      Start all VMs in the current context."
  echo "  stop-vms                       Stop all VMs in the current context."
  echo "  vmctl                          (Placeholder) Suggests using Tofu for VM control."
  echo ""
  echo "Kubernetes Management:"
  echo "  bootstrap                      Bootstrap a complete Kubernetes cluster on deployed VMs"
  echo "  get-kubeconfig                 Retrieve and merge Kubernetes cluster config into local kubeconfig."
  echo "  prepare-node <node>            Install Kubernetes components on a new VM before joining cluster."
  echo "  update-inventory               Update Ansible inventory cache from current cluster state."
  echo "  add-nodes                      Add new worker nodes to the cluster."
  echo "  remove-nodes                   Remove nodes from the Kubernetes cluster."
  echo "  drain-node <node_name>         Drain workloads from a node."
  echo "  upgrade-node <node_name>       Upgrade Kubernetes on a specific node."
  echo "  reset-node <node_name>         Reset Kubernetes on a specific node."
  echo "  reset-all-nodes                Reset Kubernetes on all nodes in the current context."
  echo "  upgrade-addons                 Install/upgrade cluster addons with interactive menu (CNI, MetalLB, cert-manager, ArgoCD, etc.)."
  echo "  configure-coredns              Configure CoreDNS to forward local domain queries to Pi-hole DNS server."
  echo "  upgrade-k8s                    Upgrade Kubernetes control plane."
  echo ""
  echo "Use 'cpc <command> --help' for more details on a specific command."
}

# --- Main Script Logic ---

# Ensure config directory exists
mkdir -p "$CONFIG_DIR"

# Check for essential commands early
check_required_commands "ansible-playbook" "ansible-inventory" "tofu" "kubectl" "jq"

COMMAND="$1"
shift # Remove command from arguments, rest are options

# Load REPO_PATH if not doing setup
if [[ "$COMMAND" != "setup-cpc" && "$COMMAND" != "" && "$COMMAND" != "-h" && "$COMMAND" != "--help" && "$COMMAND" != "help" ]]; then # Changed from setup-ccr
  REPO_PATH=$(get_repo_path)
  export REPO_PATH
  # Load environment variables from cpc.env
  load_env_vars # Will now use CPC_ENV_FILE
fi


case "$COMMAND" in
  setup-cpc) # Changed from setup-ccr
    current_script_path="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    echo "$current_script_path" > "$REPO_PATH_FILE"
    echo -e "${GREEN}cpc setup complete. Repository path set to: $current_script_path${ENDCOLOR}" # Changed from ccr
    echo -e "${BLUE}You might want to add this script to your PATH, e.g., by creating a symlink in /usr/local/bin/cpc${ENDCOLOR}" # Changed from ccr
    echo -e "${BLUE}Example: sudo ln -s "$current_script_path/cpc" /usr/local/bin/cpc${ENDCOLOR}" # Changed from ccr
    echo -e "${BLUE}Also, create a 'cpc.env' file in '$current_script_path' for version management (see cpc.env.example).${ENDCOLOR}" # Changed from ccr.env and ccr.env.example
    ;;

  get-kubeconfig)
    cpc_k8s_cluster get-kubeconfig "$@"
    ;;

  add-vm)
    cpc_proxmox add-vm "$@"
    ;;

  remove-vm)
    cpc_proxmox remove-vm "$@"
    ;;

  ctx)
    if [ -z "$1" ]; then
      current_ctx=$(get_current_cluster_context)
      echo "Current cluster context: $current_ctx"
      echo "Available Tofu workspaces:" # Changed from Terraform
      (cd "$REPO_PATH/terraform" && tofu workspace list) # Changed from terraform
      exit 0
    elif [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc ctx [<cluster_name>]" # Changed from ccr
      echo "Sets the current cluster context for cpc and switches Tofu workspace." # Changed from ccr and Terraform
      exit 0
    fi
    cluster_name="$1"
    echo "$cluster_name" > "$CLUSTER_CONTEXT_FILE"
    echo -e "${GREEN}Cluster context set to: $cluster_name${ENDCOLOR}"
    pushd "$REPO_PATH/terraform" > /dev/null || exit 1
    if tofu workspace list | grep -qw "$cluster_name"; then # Changed from terraform
      tofu workspace select "$cluster_name" # Changed from terraform
    else
      echo -e "${YELLOW}Tofu workspace '$cluster_name' does not exist. Creating and selecting.${ENDCOLOR}" # Changed from Terraform
      tofu workspace new "$cluster_name" # Changed from terraform
    fi
    popd > /dev/null || exit 1
    
    # Update template variables for the new workspace context
    set_workspace_template_vars "$cluster_name"
    ;;
  
  clone-workspace)
    if [[ "$1" == "-h" || "$1" == "--help" || $# -lt 2 ]]; then
      echo "Usage: cpc clone-workspace <source_workspace> <destination_workspace> [release_letter]"
      echo "Clones a workspace environment to create a new one."
      echo ""
      echo "Arguments:"
      echo "  <source_workspace>      Source workspace to clone (e.g., ubuntu, debian)"
      echo "  <destination_workspace> New workspace name (e.g., k8s129, test-workspace)"
      echo "  [release_letter]        Optional: Single letter to use for hostnames (defaults to first letter of destination)"
      echo ""
      echo "Example:"
      echo "  cpc clone-workspace ubuntu k8s129 k"
      exit 0
    fi

    source_workspace="$1"
    destination_workspace="$2"
    
    # Default release letter to first character of destination workspace
    if [ -z "$3" ]; then
      release_letter="${destination_workspace:0:1}"
    else
      release_letter="$3"
    fi
    
    # Validate release letter is a single character
    if [ ${#release_letter} -ne 1 ]; then
      echo -e "${RED}Error: Release letter must be a single character.${ENDCOLOR}"
      exit 1
    fi
    
    # Check if release letter is already used by another workspace
    locals_tf="$REPO_PATH/terraform/locals.tf"
    existing_release_letters=$(grep -A 50 "release_letters_map = {" "$locals_tf" | grep -E '= "[a-zA-Z]"' | grep -v "$destination_workspace" | sed -E 's/.*= "([a-zA-Z])".*/\1/g')
    
    if echo "$existing_release_letters" | grep -q "$release_letter"; then
      echo -e "${RED}Error: Release letter '$release_letter' is already used by another workspace.${ENDCOLOR}"
      echo -e "${BLUE}Please choose a different release letter. Currently used letters:${ENDCOLOR}"
      echo "$existing_release_letters" | tr '\n' ' '
      echo ""
      exit 1
    fi
    
    # Check if source workspace exists
    source_env="$REPO_PATH/envs/$source_workspace.env"
    if [ ! -f "$source_env" ]; then
      echo -e "${RED}Error: Source workspace $source_workspace does not exist.${ENDCOLOR}"
      echo -e "${BLUE}Available workspaces:${ENDCOLOR}"
      ls -1 "$REPO_PATH/envs/" | grep -E '\.env$' | sed 's/\.env$//'
      exit 1
    fi
    
    # Check if destination workspace already exists
    dest_env_file="$REPO_PATH/envs/$destination_workspace.env"
    if [ -f "$dest_env_file" ]; then
      echo -e "${RED}Error: Destination workspace $destination_workspace already exists.${ENDCOLOR}"
      exit 1
    fi
    
    # Copy environment file
    cp "$source_env" "$dest_env_file"
    if [ $? -ne 0 ]; then
      echo -e "${RED}Error: Failed to copy environment file.${ENDCOLOR}"
      exit 1
    fi
    
    # Add/update RELEASE_LETTER in the new environment file
    if grep -q "^RELEASE_LETTER=" "$dest_env_file"; then
      sed -i "s/^RELEASE_LETTER=.*/RELEASE_LETTER=$release_letter/" "$dest_env_file"
    else
      echo -e "\n# Release letter used for hostname generation" >> "$dest_env_file"
      echo "RELEASE_LETTER=$release_letter" >> "$dest_env_file"
    fi
    
    # Update locals.tf
    locals_tf="$REPO_PATH/terraform/locals.tf"
    
    # Add template VM ID mapping
    pushd "$REPO_PATH/terraform" > /dev/null || exit 1
    source_template_var=""
    
    # Add the destination workspace to template_vm_ids
    # First, get the correct variable for the source workspace
    if grep -q "\"$source_workspace\".*var.pm_template_${source_workspace}_id" "$locals_tf"; then
      # Source workspace uses its own template variable
      template_var="var.pm_template_${source_workspace}_id"
    else
      # Source workspace likely uses another OS's template
      template_var="var.pm_template_ubuntu_id"  # Default to Ubuntu template
    fi
    
    # Now add the entry to template_vm_ids
    sed -i "/^[[:space:]]*template_vm_ids = {/a \\    \"$destination_workspace\" = $template_var  # Auto-added by clone-workspace" "$locals_tf"
    echo -e "${BLUE}Added template_vm_ids entry: \"$destination_workspace\" = $template_var${ENDCOLOR}"
    
    if [ -z "$source_template_var" ]; then
      echo -e "${YELLOW}Warning: Could not find template_vm_ids entry for $source_workspace in locals.tf.${ENDCOLOR}"
      echo -e "${YELLOW}You may need to manually update locals.tf to map the template VM ID for $destination_workspace.${ENDCOLOR}"
    fi
    
    # Add release letter mapping
    sed -i "/release_letters_map = {/a \    \"$destination_workspace\" = \"$release_letter\"  # Auto-added by clone-workspace" "$locals_tf"
    
    # Add VM ID range
    # Find all existing ranges and generate a safe new one
    used_ranges=()
    while read -r line; do
      if [[ "$line" =~ \"[^\"]+\"[[:space:]]*=[[:space:]]*([0-9]+) ]]; then
        range=${BASH_REMATCH[1]}
        used_ranges+=("$range")
      fi
    done < <(grep -A 20 "vm_id_ranges = {" "$locals_tf" | grep -E '= [0-9]+')
    
    # Find a safe new range that doesn't conflict
    new_range=700
    while [[ " ${used_ranges[*]} " =~ " ${new_range} " ]]; do
      new_range=$((new_range + 100))
    done
    sed -i "/vm_id_ranges = {/a \    \"$destination_workspace\" = $new_range  # Auto-added by clone-workspace" "$locals_tf"
    
    # Add to workspace_ip_map with next available IP index
    used_ip_indices=()
    while IFS= read -r line; do
      if [[ "$line" =~ \"[^\"]+\"[[:space:]]*=[[:space:]]*([0-9]+) ]]; then
        ip_index=${BASH_REMATCH[1]}
        used_ip_indices+=("$ip_index")
      fi
    done < <(grep -A 20 "workspace_ip_map = {" "$locals_tf" | grep -E '= [0-9]+')
    
    # Find next available IP index starting from 1
    new_ip_index=1
    while [[ " ${used_ip_indices[*]} " =~ " ${new_ip_index} " ]]; do
      new_ip_index=$((new_ip_index + 1))
    done
    
    # Calculate IP range for this workspace
    ip_start=$((110 + (new_ip_index * 10)))
    ip_end=$((ip_start + 9))
    
    sed -i "/workspace_ip_map = {/a \    \"$destination_workspace\" = $new_ip_index  # IP block #$new_ip_index: 10.10.10.$ip_start-$ip_end" "$locals_tf"
    
    echo -e "${BLUE}Assigned IP block #$new_ip_index (10.10.10.$ip_start-$ip_end) to workspace '$destination_workspace'${ENDCOLOR}"
    
    echo -e "${BLUE}Updated locals.tf with new workspace entries${ENDCOLOR}"
    
    # Update validations.tf if it exists
    validations_tf="$REPO_PATH/terraform/validations.tf"
    if [ -f "$validations_tf" ]; then
      # Find the line with the condition that contains the list of valid workspaces
      condition_line=$(grep -n "condition.*contains.*terraform.workspace" "$validations_tf" | head -1 | cut -d':' -f1)
      if [ -n "$condition_line" ]; then
        # Get the last workspace in the list
        last_workspace=$(grep -o "\"[^\"]*\"" "$validations_tf" | tail -1 | tr -d '"')
        if [ -n "$last_workspace" ]; then
          # Add the new workspace to the condition
          sed -i "${condition_line}s/\"$last_workspace\"/\"$last_workspace\", \"$destination_workspace\"/" "$validations_tf"
          echo -e "${BLUE}Updated validations.tf with new workspace${ENDCOLOR}"
        else
          echo -e "${YELLOW}Warning: Could not find workspace list in validations.tf to update.${ENDCOLOR}"
        fi
      else
        echo -e "${YELLOW}Warning: Could not find condition line in validations.tf to update.${ENDCOLOR}"
      fi
    fi
    
    # Create the workspace
    if ! tofu workspace list | grep -qw "$destination_workspace"; then
      echo -e "${BLUE}Creating new Tofu workspace: $destination_workspace${ENDCOLOR}"
      tofu workspace new "$destination_workspace"
    fi
    popd > /dev/null || exit 1
    
    echo -e "${GREEN}Successfully created new workspace environment: $destination_workspace${ENDCOLOR}"
    echo -e "${BLUE}You can now edit $dest_env_file to customize your new workspace.${ENDCOLOR}"
    echo -e "${BLUE}To use this workspace, run: cpc ctx $destination_workspace${ENDCOLOR}"
    ;;
    
  delete-workspace)
    if [[ "$1" == "-h" || "$1" == "--help" || -z "$1" ]]; then
      echo "Usage: cpc delete-workspace <workspace_name> [--force]"
      echo "Deletes a workspace environment and removes it from the Terraform configuration."
      echo ""
      echo "Arguments:"
      echo "  <workspace_name>    Name of the workspace to delete (e.g., k8s129, test-workspace)"
      echo "  --force             Skip confirmation prompt"
      echo ""
      echo "Example:"
      echo "  cpc delete-workspace test-workspace"
      exit 0
    fi
    
    workspace_name="$1"
    force=false
    if [[ "$2" == "--force" ]]; then
      force=true
    fi
    
    # Check if workspace exists
    env_file="$REPO_PATH/envs/$workspace_name.env"
    if [ ! -f "$env_file" ]; then
      echo -e "${RED}Error: Workspace '$workspace_name' not found.${ENDCOLOR}"
      echo -e "${BLUE}Available workspaces:${ENDCOLOR}"
      ls -1 "$REPO_PATH/envs/" | grep -E '\.env$' | sed 's/\.env$//'
      exit 1
    fi
    
    # Check if it's one of the predefined workspaces
    if [[ "$workspace_name" == "debian" || "$workspace_name" == "ubuntu" || "$workspace_name" == "rocky" || "$workspace_name" == "suse" ]]; then
      echo -e "${RED}Error: Cannot delete predefined workspace '$workspace_name'.${ENDCOLOR}"
      echo -e "${BLUE}These are base workspaces used as templates for cloning.${ENDCOLOR}"
      exit 1
    fi
    
    # Confirm deletion unless --force is used
    if [ "$force" = false ]; then
      echo -e "${YELLOW}Warning: You are about to delete workspace '$workspace_name'.${ENDCOLOR}"
      echo -e "${YELLOW}This will:${ENDCOLOR}"
      echo -e "${YELLOW} - Delete the environment file at $env_file${ENDCOLOR}"
      echo -e "${YELLOW} - Remove entries from locals.tf${ENDCOLOR}"
      echo -e "${YELLOW} - Remove it from validations.tf${ENDCOLOR}"
      echo -e "${YELLOW} - Delete the Tofu workspace if it exists${ENDCOLOR}"
      echo -e "${YELLOW} - Stop and destroy all VMs in this workspace${ENDCOLOR}"
      echo -e "${YELLOW} - Remove all DNS records from Pi-hole for these VMs${ENDCOLOR}"
      echo -e "${YELLOW} - Clean up cloud-init snippet files${ENDCOLOR}"
      read -p "Are you sure you want to continue? (y/N) " confirm
      if [[ "$confirm" != "y" && "$confirm" != "Y" ]]; then
        echo -e "${BLUE}Operation cancelled.${ENDCOLOR}"
        exit 0
      fi
    fi
    
    # 1. First, prepare to clean up resources
    pushd "$REPO_PATH/terraform" > /dev/null || { echo -e "${RED}Failed to change to terraform directory${ENDCOLOR}"; exit 1; }
    
    # Check if the workspace exists
    if tofu workspace list | grep -qw "$workspace_name"; then
      # Save current workspace to return to it later
      original_ws=$(tofu workspace show)
      
      # Switch to the workspace we're about to delete
      if [ "$original_ws" != "$workspace_name" ]; then
        echo -e "${BLUE}Switching to workspace '$workspace_name' to check resources...${ENDCOLOR}"
        tofu workspace select "$workspace_name"
      fi
      
      # 2. Check if there are any resources in this workspace
      echo -e "${BLUE}Checking for existing resources in workspace '$workspace_name'...${ENDCOLOR}"
      tofu show -json > /dev/null 2>&1
      if [ $? -eq 0 ]; then
        # Get state info without creating anything
        resource_count=$(tofu state list 2>/dev/null | wc -l)
        if [ "$resource_count" -gt 0 ]; then
          echo -e "${BLUE}Found $resource_count resources in workspace '$workspace_name'. Proceeding with cleanup...${ENDCOLOR}"
          
          # 3. Remove DNS records from Pi-hole before destroying VMs
          echo -e "${BLUE}Removing DNS records from Pi-hole...${ENDCOLOR}"
          # Change directory to run the Python script with correct paths
          pushd "$REPO_PATH" > /dev/null || exit 1
          if [ -x "$REPO_PATH/scripts/add_pihole_dns.py" ]; then
            "$REPO_PATH/scripts/add_pihole_dns.py" --action "unregister-dns" --secrets-file "$REPO_PATH/terraform/secrets.sops.yaml" --tf-dir "$REPO_PATH/terraform"
            if [ $? -ne 0 ]; then
              echo -e "${YELLOW}Warning: Failed to remove some DNS records from Pi-hole. Continuing with cleanup...${ENDCOLOR}"
            else
              echo -e "${BLUE}Successfully removed DNS records from Pi-hole.${ENDCOLOR}"
            fi
          else
            echo -e "${YELLOW}Warning: Pi-hole DNS update script not found or not executable. DNS records may remain.${ENDCOLOR}"
          fi
          popd > /dev/null || exit 1
          
          # 4. Destroy all resources directly
          echo -e "${BLUE}Destroying all resources in workspace '$workspace_name'...${ENDCOLOR}"
          tofu destroy -auto-approve
          if [ $? -ne 0 ]; then
            echo -e "${YELLOW}Warning: Failed to destroy some resources. Manual cleanup may be required.${ENDCOLOR}"
          else
            echo -e "${BLUE}Successfully destroyed all resources in workspace '$workspace_name'.${ENDCOLOR}"
          fi
        else
          echo -e "${BLUE}No resources found in workspace '$workspace_name', skipping resource cleanup.${ENDCOLOR}"
        fi
      else
        echo -e "${BLUE}Workspace '$workspace_name' appears to be empty or has no state, skipping resource cleanup.${ENDCOLOR}"
      fi
      
      # Return to original workspace if it was different
      if [ "$original_ws" != "$workspace_name" ] && [ "$original_ws" != "default" ]; then
        echo -e "${BLUE}Switching back to workspace '$original_ws'...${ENDCOLOR}"
        tofu workspace select "$original_ws"
      else
        echo -e "${BLUE}Switching to default workspace...${ENDCOLOR}"
        tofu workspace select default
      fi
      
      # 5. Now delete the workspace
      echo -e "${BLUE}Deleting Tofu workspace '$workspace_name'...${ENDCOLOR}"
      tofu workspace delete "$workspace_name"
      echo -e "${BLUE}- Deleted Tofu workspace '$workspace_name'${ENDCOLOR}"
    else
      echo -e "${YELLOW}- Tofu workspace '$workspace_name' does not exist, skipping resource cleanup${ENDCOLOR}"
    fi
    
    # 6. Update locals.tf
    locals_tf="$REPO_PATH/terraform/locals.tf"
    
    # Remove from template_vm_ids
    sed -i "/\"$workspace_name\"[[:space:]]*=[[:space:]]*var\.[^,]*[[:space:]]*#.*clone-workspace/d" "$locals_tf"
    
    # Remove from release_letters_map
    sed -i "/\"$workspace_name\"[[:space:]]*=[[:space:]]*\"[^\"]*\"[[:space:]]*#.*clone-workspace/d" "$locals_tf"
    
    # Remove from vm_id_ranges
    sed -i "/\"$workspace_name\"[[:space:]]*=[[:space:]]*[0-9][0-9]*[[:space:]]*#.*clone-workspace/d" "$locals_tf"
    
    # Remove from workspace_ip_map (IP block assignments)
    sed -i "/\"$workspace_name\"[[:space:]]*=[[:space:]]*[0-9][0-9]*[[:space:]]*#.*IP block/d" "$locals_tf"
    
    echo -e "${BLUE}- Removed entries from locals.tf${ENDCOLOR}"
    
    # 7. Update validations.tf
    validations_tf="$REPO_PATH/terraform/validations.tf"
    if [ -f "$validations_tf" ]; then
      # Find the line with the condition that contains the list of valid workspaces
      condition_line=$(grep -n "condition.*contains.*terraform.workspace" "$validations_tf" | head -1 | cut -d':' -f1)
      if [ -n "$condition_line" ]; then
        # Remove the workspace from the list of valid workspaces
        # This is a bit more complex as we need to handle different formats
        sed -i "${condition_line}s/\"$workspace_name\"[[:space:]]*,//g" "$validations_tf" # Remove with trailing comma
        sed -i "${condition_line}s/,[[:space:]]*\"$workspace_name\"//g" "$validations_tf" # Remove with leading comma
        echo -e "${BLUE}- Updated validations.tf to remove '$workspace_name' workspace${ENDCOLOR}"
      else
        echo -e "${YELLOW}Warning: Could not find condition line in validations.tf to update.${ENDCOLOR}"
      fi
    fi
    
    # 8. Clean up cloud-init snippet files
    # First, we need to get the release letter for this workspace to know which files to delete
    if [ -f "$env_file" ]; then
      # Read RELEASE_LETTER from the environment file before we delete it
      workspace_release_letter=$(grep "^RELEASE_LETTER=" "$env_file" | cut -d'=' -f2 | tr -d '"' | tr -d "'" | head -1)
      
      # If not found in file, use fallback mapping
      if [ -z "$workspace_release_letter" ]; then
        case "$workspace_name" in
          debian) workspace_release_letter="d" ;;
          ubuntu) workspace_release_letter="u" ;;
          rocky) workspace_release_letter="r" ;;
          suse) workspace_release_letter="s" ;;
          *) workspace_release_letter=$(echo "$workspace_name" | head -c 1) ;;
        esac
      fi
      
      # Clean up cloud-init snippet files with this release letter
      snippets_dir="$REPO_PATH/terraform/snippets"
      if [ -d "$snippets_dir" ] && [ -n "$workspace_release_letter" ]; then
        echo -e "${BLUE}Cleaning up cloud-init snippet files for release letter '$workspace_release_letter'...${ENDCOLOR}"
        
        # Remove controlplane snippet files: node-c<letter><number>-userdata.yaml
        find "$snippets_dir" -name "node-c${workspace_release_letter}[0-9]*-userdata.yaml" -delete 2>/dev/null
        
        # Remove worker snippet files: node-w<letter><number>-userdata.yaml  
        find "$snippets_dir" -name "node-w${workspace_release_letter}[0-9]*-userdata.yaml" -delete 2>/dev/null
        
        # Count remaining snippet files to report
        remaining_snippets=$(find "$snippets_dir" -name "node-*${workspace_release_letter}[0-9]*-userdata.yaml" 2>/dev/null | wc -l)
        if [ "$remaining_snippets" -eq 0 ]; then
          echo -e "${BLUE}- Cleaned up all snippet files for release letter '$workspace_release_letter'${ENDCOLOR}"
        else
          echo -e "${YELLOW}- Warning: $remaining_snippets snippet files with release letter '$workspace_release_letter' still remain${ENDCOLOR}"
        fi
      else
        echo -e "${YELLOW}- Warning: Could not determine release letter for workspace '$workspace_name', skipping snippet cleanup${ENDCOLOR}"
      fi
    fi
    
    # 9. Delete the environment file last (after we're done using any values from it)
    popd > /dev/null || true  # Use true instead of exit 1 to avoid exiting on error
    rm -f "$env_file"
    echo -e "${BLUE}- Deleted environment file: $env_file${ENDCOLOR}"
    
    # Note: No need for another popd here as we've already returned to the original directory
    
    echo -e "${GREEN}Successfully deleted workspace '$workspace_name'.${ENDCOLOR}"
    ;;
  
  template)
    # Ensure workspace-specific template variables are set
    current_ctx=$(get_current_cluster_context)
    set_workspace_template_vars "$current_ctx"
    
    # Check if essential template variables are set
    if [ -z "$TEMPLATE_VM_ID" ] || [ -z "$TEMPLATE_VM_NAME" ] || [ -z "$IMAGE_NAME" ] || [ -z "$IMAGE_LINK" ]; then
      echo -e "${RED}Error: Template variables not properly set for workspace '$current_ctx'.${ENDCOLOR}"
      echo -e "${RED}Please ensure cpc.env contains the required TEMPLATE_VM_ID_*, TEMPLATE_VM_NAME_*, IMAGE_NAME_*, IMAGE_LINK_* variables.${ENDCOLOR}"
      exit 1
    fi
    
    (
      "$REPO_PATH/scripts/template.sh" "$@"
    )
    ;;

  load_secrets)
    echo -e "${BLUE}Loading secrets from SOPS...${ENDCOLOR}"
    load_secrets
    echo -e "${GREEN}Secrets loaded successfully!${ENDCOLOR}"
    echo -e "${BLUE}Available variables:${ENDCOLOR}"
    echo -e "${BLUE}  PROXMOX_HOST: $PROXMOX_HOST${ENDCOLOR}"
    echo -e "${BLUE}  PROXMOX_USERNAME: $PROXMOX_USERNAME${ENDCOLOR}"
    echo -e "${BLUE}  VM_USERNAME: $VM_USERNAME${ENDCOLOR}"
    echo -e "${BLUE}  VM_SSH_KEY: ${VM_SSH_KEY:0:20}...${ENDCOLOR}"
    ;;

  bootstrap)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc bootstrap [--skip-check] [--force]"
      echo ""
      echo "Bootstrap a complete Kubernetes cluster on the deployed VMs."
      echo ""
      echo "The bootstrap process includes:"
      echo "  1. Install Kubernetes components (kubelet, kubeadm, kubectl, containerd)"
      echo "  2. Initialize control plane with kubeadm"
      echo "  3. Install Calico CNI plugin"
      echo "  4. Join worker nodes to the cluster"
      echo "  5. Configure kubectl access for the cluster"
      echo ""
      echo "Options:"
      echo "  --skip-check   Skip VM connectivity check before starting"
      echo "  --force        Force bootstrap even if cluster appears already initialized"
      echo ""
      echo "Prerequisites:"
      echo "  - VMs must be deployed and accessible (use 'cpc deploy apply')"
      echo "  - SSH access configured to all nodes"
      echo "  - SOPS secrets loaded for VM authentication"
      echo ""
      echo "Example workflow:"
      echo "  cpc ctx ubuntu           # Set context"
      echo "  cpc deploy apply         # Deploy VMs"
      echo "  cpc bootstrap           # Bootstrap Kubernetes cluster"
      echo "  cpc get-kubeconfig      # Get cluster access"
      exit 0
    fi

    # Parse command line arguments
    skip_check=false
    force_bootstrap=false
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --skip-check)
          skip_check=true
          shift
          ;;
        --force)
          force_bootstrap=true
          shift
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          exit 1
          ;;
      esac
    done

    # Check if secrets are loaded
    check_secrets_loaded

    current_ctx=$(get_current_cluster_context)
    repo_root=$(get_repo_path)
    echo -e "${BLUE}Starting Kubernetes bootstrap for context '$current_ctx'...${ENDCOLOR}"

    # Verify that VMs are deployed and accessible
    if [ "$skip_check" = false ]; then
      echo -e "${BLUE}Checking VM connectivity...${ENDCOLOR}"
      pushd "$repo_root/terraform" > /dev/null || { echo -e "${RED}Failed to change to terraform directory${ENDCOLOR}"; exit 1; }
      
      # Check if we're in the right workspace
      if ! tofu workspace select "$current_ctx" &>/dev/null; then
        echo -e "${RED}Failed to select Tofu workspace '$current_ctx'${ENDCOLOR}" >&2
        echo -e "${RED}Please ensure VMs are deployed with 'cpc deploy apply'${ENDCOLOR}" >&2
        popd > /dev/null
        exit 1
      fi

      # Check if VMs exist
      vm_ips=$(tofu output -json k8s_node_ips 2>/dev/null)
      if [ -z "$vm_ips" ] || [ "$vm_ips" = "null" ]; then
        echo -e "${RED}No VMs found in Tofu output. Please deploy VMs first with 'cpc deploy apply'${ENDCOLOR}" >&2
        popd > /dev/null
        exit 1
      fi

      popd > /dev/null

      echo -e "${GREEN}VM connectivity check passed${ENDCOLOR}"
    fi

    # Check if cluster is already initialized (unless forced)
    if [ "$force_bootstrap" = false ]; then
      echo -e "${BLUE}Checking if cluster is already initialized...${ENDCOLOR}"
      
      # Try to connect to potential control plane and check if Kubernetes is running
      repo_root=$(get_repo_path)
      pushd "$repo_root/terraform" > /dev/null || exit 1
      control_plane_ip=$(tofu output -json k8s_node_ips 2>/dev/null | jq -r 'to_entries[] | select(.key | contains("controlplane")) | .value' | head -1)
      popd > /dev/null
      
      if [ -n "$control_plane_ip" ] && [ "$control_plane_ip" != "null" ]; then
        # Check if kubeconfig exists on control plane
        ansible_dir="$repo_root/ansible"
        remote_user=$(grep -Po '^remote_user\s*=\s*\K.*' "$ansible_dir/ansible.cfg" 2>/dev/null || echo 'root')
        
        if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o UserKnownHostsFile=/dev/null \
             "${remote_user}@${control_plane_ip}" \
             "test -f /etc/kubernetes/admin.conf" 2>/dev/null; then
          echo -e "${YELLOW}Kubernetes cluster appears to already be initialized on $control_plane_ip${ENDCOLOR}"
          echo -e "${YELLOW}Use --force to bootstrap anyway (this will reset the cluster)${ENDCOLOR}"
          exit 1
        fi
      fi
    fi

    # Run the bootstrap playbooks
    echo -e "${GREEN}Starting Kubernetes cluster bootstrap...${ENDCOLOR}"
    ansible_dir="$repo_root/ansible"
    inventory_file="$ansible_dir/inventory/tofu_inventory.py"

    # Check if inventory exists
    if [ ! -f "$inventory_file" ]; then
      echo -e "${RED}Ansible inventory not found at $inventory_file${ENDCOLOR}" >&2
      exit 1
    fi

    # First, verify connectivity to all nodes
    echo -e "${BLUE}Testing Ansible connectivity to all nodes...${ENDCOLOR}"
    if ! ansible all -i "$inventory_file" -m ping --ssh-extra-args="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"; then
      echo -e "${RED}Failed to connect to all nodes via Ansible${ENDCOLOR}" >&2
      echo -e "${RED}Please check SSH access and ensure VMs are running${ENDCOLOR}" >&2
      exit 1
    fi

    echo -e "${GREEN}Ansible connectivity test passed${ENDCOLOR}"

    # Step 1: Install Kubernetes components
    echo -e "${BLUE}Step 1: Installing Kubernetes components (kubelet, kubeadm, kubectl, containerd)...${ENDCOLOR}"
    if ! ansible_run_playbook "install_kubernetes_cluster.yml"; then
      echo -e "${RED}Failed to install Kubernetes components${ENDCOLOR}" >&2
      exit 1
    fi

    # Step 2: Initialize cluster and setup CNI with DNS hostname support
    echo -e "${BLUE}Step 2: Initializing Kubernetes cluster with DNS hostname support and installing Calico CNI...${ENDCOLOR}"
    if ! ansible_run_playbook "initialize_kubernetes_cluster_with_dns.yml"; then
      echo -e "${RED}Failed to initialize Kubernetes cluster with DNS support${ENDCOLOR}" >&2
      exit 1
    fi

    # Step 3: Validate cluster
    echo -e "${BLUE}Step 3: Validating cluster installation...${ENDCOLOR}"
    if ! ansible_run_playbook "validate_cluster.yml" -l control_plane; then
      echo -e "${YELLOW}Cluster validation failed, but continuing...${ENDCOLOR}"
    fi

    echo -e "${GREEN}Kubernetes cluster bootstrap completed successfully!${ENDCOLOR}"
    echo -e "${BLUE}Next steps:${ENDCOLOR}"
    echo -e "${BLUE}  1. Get cluster access: cpc get-kubeconfig${ENDCOLOR}"
    echo -e "${BLUE}  2. Install addons: cpc upgrade-addons${ENDCOLOR}"
    echo -e "${BLUE}  3. Verify cluster: kubectl get nodes -o wide${ENDCOLOR}"
    ;;

  run-playbook)
    if [[ "$1" == "-h" || "$1" == "--help" || -z "$1" ]]; then
      echo "Usage: cpc run-playbook <playbook_name> [ansible_options...]"
      echo ""
      echo "Run any Ansible playbook from the ansible/playbooks/ directory."
      echo ""
      echo "Arguments:"
      echo "  <playbook_name>     Name of the playbook (with or without .yml extension)"
      echo "  [ansible_options]   Additional ansible-playbook options (optional)"
      echo ""
      echo "Examples:"
      echo "  cpc run-playbook install_kubernetes_cluster"
      echo "  cpc run-playbook pb_run_command -e 'shell_command=\"uptime\"'"
      echo "  cpc run-playbook validate_cluster -l control_plane"
      echo "  cpc run-playbook configure_coredns_local_domains --check"
      echo ""
      echo "Available playbooks:"
      if [[ -d "$REPO_PATH/ansible/playbooks" ]]; then
        ls -1 "$REPO_PATH/ansible/playbooks"/*.yml 2>/dev/null | xargs -I {} basename {} .yml | sed 's/^/  /'
      else
        echo "  (Unable to list - ansible/playbooks directory not found)"
      fi
      echo ""
      echo "The command automatically:"
      echo "  - Uses the correct inventory (tofu_inventory.py)"
      echo "  - Sets current cluster context and kubernetes version"
      echo "  - Configures SSH options for seamless connection"
      exit 0
    fi

    # Get playbook name and add .yml extension if not present
    playbook_name="$1"
    shift
    if [[ "$playbook_name" != *.yml ]]; then
      playbook_name="${playbook_name}.yml"
    fi

    # Check if playbook exists
    if [[ ! -f "$REPO_PATH/ansible/playbooks/$playbook_name" ]]; then
      echo -e "${RED}Error: Playbook '$playbook_name' not found in ansible/playbooks/${ENDCOLOR}" >&2
      echo -e "${YELLOW}Available playbooks:${ENDCOLOR}" >&2
      if [[ -d "$REPO_PATH/ansible/playbooks" ]]; then
        ls -1 "$REPO_PATH/ansible/playbooks"/*.yml 2>/dev/null | xargs -I {} basename {} | sed 's/^/  /' >&2
      fi
      exit 1
    fi

    echo -e "${BLUE}Running playbook: $playbook_name${ENDCOLOR}"
    if ! ansible_run_playbook "$playbook_name" "$@"; then
      echo -e "${RED}Playbook execution failed!${ENDCOLOR}" >&2
      exit 1
    fi
    echo -e "${GREEN}Playbook '$playbook_name' completed successfully!${ENDCOLOR}"
    ;;

  clear-ssh-hosts)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc clear-ssh-hosts [--all] [--dry-run]"
      echo ""
      echo "Clear VM IP addresses from ~/.ssh/known_hosts to resolve SSH key conflicts"
      echo "when VMs are recreated with the same IP addresses but new SSH keys."
      echo ""
      echo "Options:"
      echo "  --all       Clear all VM IPs from all contexts (not just current)"
      echo "  --dry-run   Show what would be removed without actually removing"
      echo ""
      echo "The command will:"
      echo "  1. Get VM IP addresses from current Terraform/Tofu outputs"
      echo "  2. Remove matching entries from ~/.ssh/known_hosts"
      echo "  3. Display summary of removed entries"
      echo ""
      echo "Example usage:"
      echo "  cpc clear-ssh-hosts           # Clear IPs from current context"
      echo "  cpc clear-ssh-hosts --all     # Clear IPs from all contexts"
      echo "  cpc clear-ssh-hosts --dry-run # Preview what would be removed"
      exit 0
    fi

    # Parse command line arguments
    clear_all=false
    dry_run=false
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --all)
          clear_all=true
          shift
          ;;
        --dry-run)
          dry_run=true
          shift
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          echo "Use 'cpc clear-ssh-hosts --help' for usage information."
          exit 1
          ;;
      esac
    done

    # Check if ~/.ssh/known_hosts exists
    if [ ! -f ~/.ssh/known_hosts ]; then
      echo -e "${YELLOW}No ~/.ssh/known_hosts file found. Nothing to clear.${ENDCOLOR}"
      exit 0
    fi

    current_ctx=$(get_current_cluster_context)
    repo_root=$(get_repo_path)
    
    echo -e "${BLUE}Clearing SSH known_hosts entries for VM IP addresses...${ENDCOLOR}"
    
    # Function to get VM IPs from a specific context
    get_vm_ips_from_context() {
      local context="$1"
      local terraform_dir="$repo_root/terraform"
      
      # Change to terraform directory and select workspace
      pushd "$terraform_dir" > /dev/null || return 1
      
      # Save current workspace before switching
      local original_workspace
      original_workspace=$(tofu workspace show 2>/dev/null)
      
      if ! tofu workspace select "$context" &>/dev/null; then
        echo -e "${YELLOW}Warning: Could not select Tofu workspace '$context'${ENDCOLOR}" >&2
        popd > /dev/null
        return 1
      fi
      
      # Get VM IPs from Tofu output
      local vm_ips
      vm_ips=$(tofu output -json k8s_node_ips 2>/dev/null)
      
      # Restore original workspace
      if [ -n "$original_workspace" ] && [ "$original_workspace" != "$context" ]; then
        tofu workspace select "$original_workspace" &>/dev/null
      fi
      
      popd > /dev/null
      
      if [ -z "$vm_ips" ] || [ "$vm_ips" = "null" ] || [ "$vm_ips" = "{}" ]; then
        return 1
      fi
      
      # Extract IP addresses from JSON output
      echo "$vm_ips" | jq -r 'to_entries[] | .value' 2>/dev/null | grep -E '^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$'
    }
    
    # Collect all VM IPs to remove
    vm_ips_to_clear=()
    vm_hostnames_to_clear=()
    
    if [ "$clear_all" = true ]; then
      echo -e "${BLUE}Collecting VM IPs from all contexts...${ENDCOLOR}"
      
      # Get all available workspaces
      pushd "$repo_root/terraform" > /dev/null || { echo -e "${RED}Failed to access terraform directory${ENDCOLOR}"; exit 1; }
      workspaces=$(tofu workspace list | grep -v '^\*' | sed 's/^[ *]*//' | grep -v '^default$')
      popd > /dev/null
      
      for workspace in $workspaces; do
        echo -e "${BLUE}  Checking context: $workspace${ENDCOLOR}"
        ips=$(get_vm_ips_from_context "$workspace")
        if [ -n "$ips" ]; then
          while IFS= read -r ip; do
            if [ -n "$ip" ]; then
              vm_ips_to_clear+=("$ip")
            fi
          done <<< "$ips"
          echo -e "${BLUE}    Found IPs: $(echo "$ips" | tr '\n' ' ')${ENDCOLOR}"
        else
          echo -e "${YELLOW}    No VMs found in context '$workspace'${ENDCOLOR}"
        fi
      done
    else
      echo -e "${BLUE}Collecting VM IPs from current context: $current_ctx${ENDCOLOR}"
      ips=$(get_vm_ips_from_context "$current_ctx")
      if [ -n "$ips" ]; then
        while IFS= read -r ip; do
          if [ -n "$ip" ]; then
            vm_ips_to_clear+=("$ip")
          fi
        done <<< "$ips"
        echo -e "${BLUE}  Found IPs: $(echo "$ips" | tr '\n' ' ')${ENDCOLOR}"
      else
        echo -e "${YELLOW}No VMs found in current context '$current_ctx'${ENDCOLOR}"
        echo -e "${BLUE}Make sure VMs are deployed with 'cpc deploy apply'${ENDCOLOR}"
        exit 0
      fi
    fi
    
    # Now get hostnames from cluster-info
    vm_hostnames_to_clear=()
    
    # Get cluster information to extract hostnames
    echo -e "${BLUE}Fetching hostname information from cluster...${ENDCOLOR}"
    cluster_info=$(get_cluster_info "json" 2>/dev/null)
    
    if [ -n "$cluster_info" ] && [ "$cluster_info" != "null" ]; then
      # Extract hostnames from cluster info
      while IFS= read -r hostname; do
        if [ -n "$hostname" ]; then
          vm_hostnames_to_clear+=("$hostname")
        fi
      done < <(echo "$cluster_info" | jq -r 'to_entries[] | .value.hostname')
      
      echo -e "${BLUE}  Found hostnames: ${vm_hostnames_to_clear[*]} ${ENDCOLOR}"
    else
      echo -e "${YELLOW}  Could not get hostnames from cluster info, using default naming pattern${ENDCOLOR}"
      
      # Add common hostname patterns based on current context
      # Hardcoded hostnames for the common pattern used in the cluster
      vm_hostnames_to_clear+=(
        "ck1.bevz.net" "wk1.bevz.net" "wk2.bevz.net" "wk3.bevz.net"
        "ck1" "wk1" "wk2" "wk3"
      )
      
      echo -e "${BLUE}  Added default hostnames: ${vm_hostnames_to_clear[*]} ${ENDCOLOR}"
    fi
    
    # Add short hostnames (without domain suffix)
    short_hostnames=()
    for hostname in "${vm_hostnames_to_clear[@]}"; do
      short_name=$(echo "$hostname" | cut -d. -f1)
      if [[ "$short_name" != "$hostname" ]]; then
        short_hostnames+=("$short_name")
      fi
    done
    
    # Add short hostnames to the list
    if [ ${#short_hostnames[@]} -gt 0 ]; then
      vm_hostnames_to_clear+=("${short_hostnames[@]}")
    fi
    
    # Remove duplicates from IPs and hostnames
    vm_ips_to_clear=($(printf '%s\n' "${vm_ips_to_clear[@]}" | sort -u))
    vm_hostnames_to_clear=($(printf '%s\n' "${vm_hostnames_to_clear[@]}" | sort -u))
    
    if [ ${#vm_ips_to_clear[@]} -eq 0 ]; then
      echo -e "${YELLOW}No VM IP addresses found to clear.${ENDCOLOR}"
      exit 0
    fi
    
    echo -e "${BLUE}VM entries to clear from ~/.ssh/known_hosts:${ENDCOLOR}"
    echo -e "${BLUE}  IP addresses:${ENDCOLOR}"
    for ip in "${vm_ips_to_clear[@]}"; do
      echo -e "${BLUE}    - $ip${ENDCOLOR}"
    done
    
    echo -e "${BLUE}  Hostnames:${ENDCOLOR}"
    for hostname in "${vm_hostnames_to_clear[@]}"; do
      echo -e "${BLUE}    - $hostname${ENDCOLOR}"
    done
    
    if [ "$dry_run" = true ]; then
      echo -e "${YELLOW}Dry run mode - showing what would be removed:${ENDCOLOR}"
      for ip in "${vm_ips_to_clear[@]}"; do
        entries=$(grep -n "^$ip " ~/.ssh/known_hosts 2>/dev/null || true)
        if [ -n "$entries" ]; then
          echo -e "${YELLOW}  Would remove entries for $ip:${ENDCOLOR}"
          echo "$entries" | sed 's/^/    /'
        else
          echo -e "${BLUE}  No entries found for $ip${ENDCOLOR}"
        fi
      done
      echo -e "${BLUE}Run without --dry-run to actually remove entries.${ENDCOLOR}"
      exit 0
    fi
    
    # Create backup of known_hosts
    backup_file=~/.ssh/known_hosts.backup.$(date +%Y%m%d_%H%M%S)
    cp ~/.ssh/known_hosts "$backup_file"
    echo -e "${BLUE}Created backup: $backup_file${ENDCOLOR}"
    
    # Get hostnames for each IP using DNS lookup
    declare -A ip_to_hostname
    for ip in "${vm_ips_to_clear[@]}"; do
      # Try to get hostname using reverse DNS lookup
      hostname=$(getent hosts "$ip" | awk '{print $2}' 2>/dev/null || echo "")
      if [ -n "$hostname" ]; then
        ip_to_hostname["$ip"]=$hostname
      fi
    done
    
    # Remove entries for each IP and hostname
    removed_count=0
    
    # First process IPs
    for ip in "${vm_ips_to_clear[@]}"; do
      # Count entries before removal
      before_count=$(grep -c "^$ip " ~/.ssh/known_hosts 2>/dev/null || echo "0")
      # Make sure before_count is a clean integer
      before_count=$(echo "$before_count" | tr -d '\n\r')
      
      if [ "$before_count" -gt 0 ]; then
        # Remove entries for this IP
        sed -i "/^$ip /d" ~/.ssh/known_hosts
        
        # Count entries after removal
        after_count=$(grep -c "^$ip " ~/.ssh/known_hosts 2>/dev/null || echo "0")
        # Make sure after_count is a clean integer
        after_count=$(echo "$after_count" | tr -d '\n\r')
        entries_removed=$((before_count - after_count))
        
        if [ $entries_removed -gt 0 ]; then
          echo -e "${GREEN}  Removed $entries_removed entries for IP $ip${ENDCOLOR}"
          removed_count=$((removed_count + entries_removed))
        fi
      else
        echo -e "${BLUE}  No entries found for IP $ip${ENDCOLOR}"
      fi
    done
    
    # Then process hostnames - both from the cluster and from DNS lookups
    all_hostnames=()
    
    # Add hostnames from cluster info if available
    if [ ${#vm_hostnames_to_clear[@]} -gt 0 ]; then
      for hostname in "${vm_hostnames_to_clear[@]}"; do
        all_hostnames+=("$hostname")
      done
    fi
    
    # Add hostnames from DNS lookups
    for ip in "${!ip_to_hostname[@]}"; do
      hostname="${ip_to_hostname[$ip]}"
      # Check if hostname is already in the list
      if [[ ! " ${all_hostnames[*]} " =~ " ${hostname} " ]]; then
        all_hostnames+=("$hostname")
      fi
    done
    
    # Process each hostname
    for hostname in "${all_hostnames[@]}"; do
      # Skip empty hostnames
      [ -z "$hostname" ] && continue
      
      # Count entries before removal
      before_count=$(grep -c "^$hostname " ~/.ssh/known_hosts 2>/dev/null || echo "0")
      # Make sure before_count is a clean integer
      before_count=$(echo "$before_count" | tr -d '\n\r')
      
      if [ "$before_count" -gt 0 ]; then
        # Remove entries for this hostname
        sed -i "/^$hostname /d" ~/.ssh/known_hosts
        
        # Count entries after removal
        after_count=$(grep -c "^$hostname " ~/.ssh/known_hosts 2>/dev/null || echo "0")
        # Make sure after_count is a clean integer
        after_count=$(echo "$after_count" | tr -d '\n\r')
        entries_removed=$((before_count - after_count))
        
        if [ $entries_removed -gt 0 ]; then
          echo -e "${GREEN}  Removed $entries_removed entries for hostname $hostname${ENDCOLOR}"
          removed_count=$((removed_count + entries_removed))
        fi
      else
        echo -e "${BLUE}  No entries found for hostname $hostname${ENDCOLOR}"
      fi
    done
    
    # Use ssh-keygen -R for more reliable removal of host entries
    echo -e "${BLUE}Using ssh-keygen to remove hostname entries...${ENDCOLOR}"
    
    # For IPs
    for ip in "${vm_ips_to_clear[@]}"; do
      # Try with ssh-keygen -R which handles entries better
      ssh-keygen -R "$ip" &>/dev/null
      if [ $? -eq 0 ]; then
        echo -e "${GREEN}  Removed entries for IP $ip using ssh-keygen${ENDCOLOR}"
        removed_count=$((removed_count + 1))
      fi
    done
    
    # Now use hostnames from the list we built earlier
    for hostname in "${vm_hostnames_to_clear[@]}"; do
      # Skip empty hostnames
      [ -z "$hostname" ] && continue
      
      # Try with ssh-keygen -R which handles entries better
      output=$(ssh-keygen -R "$hostname" 2>&1)
      if [ $? -eq 0 ] || [[ "$output" == *"Host $hostname found:"* ]]; then
        echo -e "${GREEN}  Removed entries for hostname $hostname using ssh-keygen${ENDCOLOR}"
        removed_count=$((removed_count + 1))
      else
        echo -e "${BLUE}  No entries found for hostname $hostname${ENDCOLOR}"
      fi
    done
    
    if [ $removed_count -gt 0 ]; then
      echo -e "${GREEN}Successfully removed $removed_count SSH known_hosts entries.${ENDCOLOR}"
      echo -e "${BLUE}Backup saved to: $backup_file${ENDCOLOR}"
    else
      echo -e "${YELLOW}No SSH known_hosts entries were removed.${ENDCOLOR}"
      # Remove backup if nothing was changed
      rm -f "$backup_file"
    fi
    
    echo -e "${BLUE}SSH known_hosts cleanup completed.${ENDCOLOR}"
    ;;

  clear-ssh-maps)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc clear-ssh-maps [--all] [--dry-run]"
      echo ""
      echo "Clear SSH control sockets and connections for VMs to resolve SSH connection issues"
      echo "when VMs are recreated or SSH configurations change."
      echo ""
      echo "Options:"
      echo "  --all       Clear SSH connections for all contexts (not just current)"
      echo "  --dry-run   Show what would be cleared without actually clearing"
      echo ""
      echo "The command will:"
      echo "  1. Get VM IP addresses from current Terraform/Tofu outputs"
      echo "  2. Close active SSH connections to those IPs"
      echo "  3. Remove SSH control sockets (if ControlMaster is enabled)"
      echo "  4. Display summary of cleared connections"
      echo ""
      echo "Example usage:"
      echo "  cpc clear-ssh-maps           # Clear SSH connections for current context"
      echo "  cpc clear-ssh-maps --all     # Clear SSH connections for all contexts"
      echo "  cpc clear-ssh-maps --dry-run # Preview what would be cleared"
      exit 0
    fi

    # Parse command line arguments
    clear_all=false
    dry_run=false
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --all)
          clear_all=true
          shift
          ;;
        --dry-run)
          dry_run=true
          shift
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          echo "Use 'cpc clear-ssh-maps --help' for usage information."
          exit 1
          ;;
      esac
    done

    current_ctx=$(get_current_cluster_context)
    repo_root=$(get_repo_path)
    
    echo -e "${BLUE}Clearing SSH connections and control sockets for VM IP addresses...${ENDCOLOR}"
    
    # Function to get VM IPs from a specific context (reuse from clear-ssh-hosts)
    get_vm_ips_from_context() {
      local context="$1"
      local terraform_dir="$repo_root/terraform"
      
      # Change to terraform directory and select workspace
      pushd "$terraform_dir" > /dev/null || return 1
      
      # Save current workspace before switching
      local original_workspace
      original_workspace=$(tofu workspace show 2>/dev/null)
      
      if ! tofu workspace select "$context" &>/dev/null; then
        echo -e "${YELLOW}Warning: Could not select Tofu workspace '$context'${ENDCOLOR}" >&2
        popd > /dev/null
        return 1
      fi
      
      # Get VM IPs from Tofu output
      local vm_ips
      vm_ips=$(tofu output -json k8s_node_ips 2>/dev/null)
      
      # Restore original workspace
      if [ -n "$original_workspace" ] && [ "$original_workspace" != "$context" ]; then
        tofu workspace select "$original_workspace" &>/dev/null
      fi
      
      popd > /dev/null
      
      if [ -z "$vm_ips" ] || [ "$vm_ips" = "null" ] || [ "$vm_ips" = "{}" ]; then
        return 1
      fi
      
      # Extract IP addresses from JSON output
      echo "$vm_ips" | jq -r 'to_entries[] | .value' 2>/dev/null | grep -E '^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$'
    }
    
    # Collect all VM IPs to clear
    vm_ips_to_clear=()
    
    if [ "$clear_all" = true ]; then
      echo -e "${BLUE}Collecting VM IPs from all contexts...${ENDCOLOR}"
      
      # Get all available workspaces
      pushd "$repo_root/terraform" > /dev/null || { echo -e "${RED}Failed to access terraform directory${ENDCOLOR}"; exit 1; }
      workspaces=$(tofu workspace list | grep -v '^\*' | sed 's/^[ *]*//' | grep -v '^default$')
      popd > /dev/null
      
      for workspace in $workspaces; do
        echo -e "${BLUE}  Checking context: $workspace${ENDCOLOR}"
        ips=$(get_vm_ips_from_context "$workspace")
        if [ -n "$ips" ]; then
          while IFS= read -r ip; do
            if [ -n "$ip" ]; then
              vm_ips_to_clear+=("$ip")
            fi
          done <<< "$ips"
          echo -e "${BLUE}    Found IPs: $(echo "$ips" | tr '\n' ' ')${ENDCOLOR}"
        else
          echo -e "${YELLOW}    No VMs found in context '$workspace'${ENDCOLOR}"
        fi
      done
    else
      echo -e "${BLUE}Collecting VM IPs from current context: $current_ctx${ENDCOLOR}"
      ips=$(get_vm_ips_from_context "$current_ctx")
      if [ -n "$ips" ]; then
        while IFS= read -r ip; do
          if [ -n "$ip" ]; then
            vm_ips_to_clear+=("$ip")
          fi
        done <<< "$ips"
        echo -e "${BLUE}  Found IPs: $(echo "$ips" | tr '\n' ' ')${ENDCOLOR}"
      else
        echo -e "${YELLOW}No VMs found in current context '$current_ctx'${ENDCOLOR}"
        echo -e "${BLUE}Make sure VMs are deployed with 'cpc deploy apply'${ENDCOLOR}"
        exit 0
      fi
    fi
    
    # Remove duplicates
    vm_ips_to_clear=($(printf '%s\n' "${vm_ips_to_clear[@]}" | sort -u))
    
    if [ ${#vm_ips_to_clear[@]} -eq 0 ]; then
      echo -e "${YELLOW}No VM IP addresses found to clear SSH connections for.${ENDCOLOR}"
      exit 0
    fi
    
    echo -e "${BLUE}VM IP addresses to clear SSH connections for:${ENDCOLOR}"
    for ip in "${vm_ips_to_clear[@]}"; do
      echo -e "${BLUE}  - $ip${ENDCOLOR}"
    done
    
    # Find SSH control sockets and active connections
    connections_found=0
    sockets_found=0
    
    if [ "$dry_run" = true ]; then
      echo -e "${YELLOW}Dry run mode - showing what would be cleared:${ENDCOLOR}"
      
      for ip in "${vm_ips_to_clear[@]}"; do
        echo -e "${BLUE}  Checking SSH connections for $ip:${ENDCOLOR}"
        
        # Check for active SSH connections
        active_connections=$(ps aux | grep -E "ssh.*$ip" | grep -v grep | grep -v "clear-ssh-maps" || true)
        if [ -n "$active_connections" ]; then
          echo -e "${YELLOW}    Active SSH connections found:${ENDCOLOR}"
          echo "$active_connections" | sed 's/^/      /'
          connections_found=$((connections_found + 1))
        fi
        
        # Check for SSH control sockets in common locations
        socket_locations=(
          "$HOME/.ssh/sockets"
          "$HOME/.ssh/connections"
          "$HOME/.ssh/master"
          "/tmp"
        )
        
        for socket_dir in "${socket_locations[@]}"; do
          if [ -d "$socket_dir" ]; then
            sockets=$(find "$socket_dir" -name "*$ip*" -type s 2>/dev/null || true)
            if [ -n "$sockets" ]; then
              echo -e "${YELLOW}    SSH control sockets found in $socket_dir:${ENDCOLOR}"
              echo "$sockets" | sed 's/^/      /'
              sockets_found=$((sockets_found + $(echo "$sockets" | wc -l)))
            fi
          fi
        done
        
        if [ -z "$active_connections" ] && [ $sockets_found -eq 0 ]; then
          echo -e "${BLUE}    No SSH connections or sockets found for $ip${ENDCOLOR}"
        fi
      done
      
      echo -e "${BLUE}Total connections to close: $connections_found${ENDCOLOR}"
      echo -e "${BLUE}Total sockets to remove: $sockets_found${ENDCOLOR}"
      echo -e "${BLUE}Run without --dry-run to actually clear connections.${ENDCOLOR}"
      exit 0
    fi
    
    # Actually clear SSH connections and sockets
    cleared_connections=0
    cleared_sockets=0
    
    for ip in "${vm_ips_to_clear[@]}"; do
      echo -e "${BLUE}  Clearing SSH connections for $ip...${ENDCOLOR}"
      
      # Kill active SSH connections
      ssh_pids=$(ps aux | grep -E "ssh.*$ip" | grep -v grep | grep -v "clear-ssh-maps" | awk '{print $2}' || true)
      if [ -n "$ssh_pids" ]; then
        for pid in $ssh_pids; do
          if kill "$pid" 2>/dev/null; then
            echo -e "${GREEN}    Closed SSH connection (PID: $pid)${ENDCOLOR}"
            cleared_connections=$((cleared_connections + 1))
          fi
        done
      fi
      
      # Remove SSH control sockets
      socket_locations=(
        "$HOME/.ssh/sockets"
        "$HOME/.ssh/connections"
        "$HOME/.ssh/master"
        "/tmp"
      )
      
      for socket_dir in "${socket_locations[@]}"; do
        if [ -d "$socket_dir" ]; then
          sockets=$(find "$socket_dir" -name "*$ip*" -type s 2>/dev/null || true)
          if [ -n "$sockets" ]; then
            while IFS= read -r socket; do
              if [ -n "$socket" ] && rm -f "$socket" 2>/dev/null; then
                echo -e "${GREEN}    Removed SSH socket: $(basename "$socket")${ENDCOLOR}"
                cleared_sockets=$((cleared_sockets + 1))
              fi
            done <<< "$sockets"
          fi
        fi
      done
    done
    
    # Also check for SSH master connections that might use different naming
    echo -e "${BLUE}Checking for SSH master connections...${ENDCOLOR}"
    for ip in "${vm_ips_to_clear[@]}"; do
      # Try to close SSH master connections using ssh -O exit
      if ssh -O check "$ip" 2>/dev/null; then
        if ssh -O exit "$ip" 2>/dev/null; then
          echo -e "${GREEN}  Closed SSH master connection to $ip${ENDCOLOR}"
          cleared_connections=$((cleared_connections + 1))
        fi
      fi
    done
    
    echo -e "${GREEN}SSH connection cleanup completed:${ENDCOLOR}"
    echo -e "${GREEN}  - Closed $cleared_connections active connections${ENDCOLOR}"
    echo -e "${GREEN}  - Removed $cleared_sockets control sockets${ENDCOLOR}"
    
    if [ $cleared_connections -eq 0 ] && [ $cleared_sockets -eq 0 ]; then
      echo -e "${YELLOW}No SSH connections or sockets were found to clear.${ENDCOLOR}"
    fi
    ;;

  add-nodes)
    cpc_k8s_nodes add-nodes "$@"
    ;;

  prepare-node)
    cpc_k8s_nodes prepare-node "$@"
    ;;

  update-inventory)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc update-inventory"
      echo ""
      echo "Update the Ansible inventory cache from current cluster state."
      echo "This command fetches the latest cluster information and updates"
      echo "the inventory cache file used by Ansible playbooks."
      echo ""
      echo "This is automatically called before Ansible operations, but can be"
      echo "run manually to troubleshoot inventory issues."
      exit 0
    fi

    echo -e "${BLUE}Updating Ansible inventory cache...${ENDCOLOR}"
    
    repo_root=$(get_repo_path)
    cache_file="$repo_root/.ansible_inventory_cache.json"
    terraform_dir="$repo_root/terraform"
    
    if [ ! -d "$terraform_dir" ]; then
      echo -e "${RED}Error: terraform directory not found at $terraform_dir${ENDCOLOR}" >&2
      exit 1
    fi

    # Export AWS credentials for terraform backend (needed for tofu output)
    export AWS_ACCESS_KEY_ID="${AWS_ACCESS_KEY_ID:-}"
    export AWS_SECRET_ACCESS_KEY="${AWS_SECRET_ACCESS_KEY:-}"
    export AWS_DEFAULT_REGION="${AWS_DEFAULT_REGION:-us-east-1}"

    # Load current cluster info using cluster-info (which handles credentials)
    echo -e "${YELLOW}Getting cluster information...${ENDCOLOR}"
    
    # Get cluster info and extract only the JSON part (last line that starts with {)
    cluster_info_output=$(./cpc cluster-info --format json 2>/dev/null)
    cluster_summary=$(echo "$cluster_info_output" | grep '^{.*}$' | tail -1)
    
    if [ -z "$cluster_summary" ] || [ "$cluster_summary" = "null" ]; then
      echo -e "${RED}Error: Could not get cluster information from terraform${ENDCOLOR}" >&2
      echo -e "${BLUE}Make sure terraform is applied and cluster is running${ENDCOLOR}"
      exit 1
    fi
    
    if [ -z "$cluster_summary" ] || [ "$cluster_summary" = "null" ]; then
      echo -e "${RED}Error: Could not get cluster information from terraform${ENDCOLOR}" >&2
      echo -e "${BLUE}Make sure terraform is applied and cluster is running${ENDCOLOR}"
      exit 1
    fi
    
    # Generate inventory from cluster_summary
    inventory_json=$(echo "$cluster_summary" | jq '{
      "_meta": {
        "hostvars": (
          to_entries | reduce .[] as $item ({}; 
            . + {
              ($item.value.IP): {
                "ansible_host": $item.value.IP,
                "node_name": $item.key,
                "hostname": $item.value.hostname,
                "vm_id": $item.value.VM_ID,
                "k8s_role": (if ($item.key | contains("controlplane")) then "control-plane" else "worker" end)
              }
            } + {
              ($item.value.hostname): {
                "ansible_host": $item.value.IP,
                "node_name": $item.key,
                "hostname": $item.value.hostname,
                "vm_id": $item.value.VM_ID,
                "k8s_role": (if ($item.key | contains("controlplane")) then "control-plane" else "worker" end)
              }
            }
          )
        )
      },
      "all": {
        "children": ["control_plane", "workers"]
      },
      "control_plane": {
        "hosts": [to_entries | map(select(.key | contains("controlplane")) | .value.IP) | .[]] + [to_entries | map(select(.key | contains("controlplane")) | .value.hostname) | .[]]
      },
      "workers": {
        "hosts": [to_entries | map(select(.key | contains("worker")) | .value.IP) | .[]] + [to_entries | map(select(.key | contains("worker")) | .value.hostname) | .[]]
      }
    }')
    
    # Write to cache file
    echo "$inventory_json" > "$cache_file"
    
    echo -e "${GREEN}Ansible inventory cache updated at $cache_file${ENDCOLOR}"
    echo -e "${BLUE}Inventory contents:${ENDCOLOR}"
    jq '.' "$cache_file"
    ;;

  remove-nodes)
    cpc_k8s_nodes remove-nodes "$@"
    ;;

  drain-node)
    cpc_k8s_nodes drain-node "$@"
    ;;

  upgrade-node)
    cpc_k8s_nodes upgrade-node "$@"
    ;;

  reset-node)
    cpc_k8s_nodes reset-node "$@"
    ;;

  reset-all-nodes)
    read -r -p "Are you sure you want to reset Kubernetes on ALL nodes in context '$(get_current_cluster_context)'? [y/N] " response
    if [[ "$response" =~ ^([yY][eE][sS]|[yY])$ ]]; then
      ansible_run_playbook "pb_reset_all_nodes.yml"
    else
      echo "Operation cancelled."
    fi
    ;;

  upgrade-addons)
    cpc_cluster_ops upgrade-addons "$@"
    ;;

  upgrade-k8s)
    if [[ "$1" == "-h" || "$1" == "--help" ]]; then
      echo "Usage: cpc upgrade-k8s [--target-version <version>] [--skip-etcd-backup]"
      echo ""
      echo "Upgrade Kubernetes control plane components."
      echo ""
      echo "Options:"
      echo "  --target-version <version>  Target Kubernetes version (default: from environment)"
      echo "  --skip-etcd-backup         Skip etcd backup before upgrade"
      echo ""
      echo "The upgrade process will:"
      echo "  1. Backup etcd (unless --skip-etcd-backup is specified)"
      echo "  2. Upgrade control plane components on each control plane node"
      echo "  3. Verify cluster health after upgrade"
      echo ""
      echo "Warning: This will upgrade the control plane. Ensure you have backups!"
      exit 0
    fi

    # Parse command line arguments
    target_version=""
    skip_etcd_backup="false"
    
    while [[ $# -gt 0 ]]; do
      case $1 in
        --target-version)
          target_version="$2"
          shift 2
          ;;
        --skip-etcd-backup)
          skip_etcd_backup="true"
          shift
          ;;
        *)
          echo -e "${RED}Unknown option: $1${ENDCOLOR}" >&2
          exit 1
          ;;
      esac
    done

    # Confirmation prompt
    current_ctx=$(get_current_cluster_context)
    echo -e "${YELLOW}Warning: This will upgrade the Kubernetes control plane for context '$current_ctx'.${ENDCOLOR}"
    read -r -p "Are you sure you want to continue? [y/N] " response
    if [[ ! "$response" =~ ^([yY][eE][sS]|[yY])$ ]]; then
      echo "Operation cancelled."
      exit 0
    fi

    extra_vars="-e skip_etcd_backup=$skip_etcd_backup"
    if [ -n "$target_version" ]; then
      extra_vars="$extra_vars -e target_k8s_version=$target_version"
    fi

    echo -e "${BLUE}Upgrading Kubernetes control plane...${ENDCOLOR}"
    ansible_run_playbook "pb_upgrade_k8s_control_plane.yml" -l control_plane -e "skip_etcd_backup=$skip_etcd_backup" $([ -n "$target_version" ] && echo "-e target_k8s_version=$target_version")
    ;;

  vmctl)
    echo -e "${BLUE}VM control (start, stop, create, delete) is primarily managed by Tofu in this project.${ENDCOLOR}" # Changed from Terraform
    echo -e "${BLUE}Please use 'tofu apply', 'tofu destroy', or modify your .tfvars and re-apply.${ENDCOLOR}" # Changed from terraform
    echo -e "${BLUE}Example: To stop a VM, you might comment it out in Tofu and apply, or use Proxmox UI/API directly.${ENDCOLOR}" # Changed from Terraform
    # Placeholder for future direct VM interactions if needed via Proxmox API etc.
    # ansible_run_playbook "pb_vm_control.yml" "localhost" "-e vm_name=$1 -e action=$2"
    ;;

  run-command)
    cpc_ansible run-command "$@"
    ;;

  dns-pihole)
    cpc_dns_pihole "$@"
    ;;

  cluster-info)
    cpc_tofu cluster-info "$@"
    ;;

  deploy)
    cpc_tofu deploy "$@"
    ;;

  generate-hostnames)
    cpc_tofu generate-hostnames "$@"
    ;;

  run-ansible)
    cpc_ansible run-ansible "$@"
    ;;

  gen_hostnames)
    cpc_tofu gen_hostnames "$@"
    ;;

  start-vms)
    cpc_tofu start-vms "$@"
    ;;

  configure-coredns)
    cpc_cluster_ops configure-coredns "$@"
    ;;

  stop-vms)
    cpc_tofu stop-vms "$@"
    ;;

  "" | "-h" | "--help" | "help")
    display_usage
    ;;

  scripts/*)
    # Handle running scripts directly: ./cpc scripts/script_name.sh
    script_path="$REPO_PATH/$COMMAND"
    if [[ -f "$script_path" && -x "$script_path" ]]; then
      echo -e "${BLUE}Running script: $script_path${ENDCOLOR}"
      # Pass all remaining arguments to the script
      "$script_path" "$@"
    elif [[ -f "$script_path" ]]; then
      echo -e "${RED}Error: Script $script_path exists but is not executable.${ENDCOLOR}" >&2
      echo -e "${BLUE}Try: chmod +x $script_path${ENDCOLOR}" >&2
      exit 1
    else
      echo -e "${RED}Error: Script not found at $script_path${ENDCOLOR}" >&2
      exit 1
    fi
    ;;

  # Legacy aliases for backward compatibility
  add-node)
    echo -e "${YELLOW}Warning: 'add-node' is deprecated. Use 'add-vm' instead.${ENDCOLOR}"
    shift
    set -- "add-vm" "$@"
    exec "$0" "$@"
    ;;

  remove-node)
    echo -e "${YELLOW}Warning: 'remove-node' is deprecated. Use 'remove-vm' instead.${ENDCOLOR}"
    shift
    set -- "remove-vm" "$@"
    exec "$0" "$@"
    ;;

  update-pihole)
    echo -e "${YELLOW}Warning: 'update-pihole' is deprecated. Use 'dns-pihole' instead.${ENDCOLOR}"
    shift
    set -- "dns-pihole" "$@"
    exec "$0" "$@"
    ;;

  delete-node)
    echo -e "${YELLOW}Warning: 'delete-node' is deprecated. Use 'remove-nodes' instead.${ENDCOLOR}"
    # Pass all arguments as is to remove-nodes
    set -- "remove-nodes" "$@"
    exec "$0" "$@"
    ;;

  *)
    echo -e "${RED}Unknown command: $COMMAND${ENDCOLOR}" >&2
    display_usage
    exit 1
    ;;
esac

exit 0
