# Project Architecture Documentation

## Overview

The **my-kthw** (Kubernetes The Hard Way) project implements a comprehensive Infrastructure as Code (IaC) solution for deploying and managing Kubernetes clusters on Proxmox VE infrastructure. The architecture follows modern DevOps practices with clear separation of concerns between infrastructure provisioning, configuration management, and cluster orchestration.

## Workspace Support

The project supports multiple Linux distributions through a workspace-based architecture:

### âœ… Fully Supported Workspaces
- **Ubuntu 24.04** (`ubuntu`): Complete functionality with all features working
  - Kubernetes v1.31.x
  - Calico v3.28.0 CNI
  - MetalLB v0.14.8
  - All addons fully tested
  
- **SUSE** (`suse`): Complete functionality with all features working  
  - Kubernetes v1.30.x
  - Calico v3.27.0 CNI
  - MetalLB v0.14.5
  - All addons fully tested

### ðŸš§ Partial Support Workspaces
- **Debian** (`debian`): Basic functionality (some features in development)
- **Rocky Linux** (`rocky`): Basic functionality (some features in development)

Each workspace maintains its own:
- VM template configuration
- Kubernetes version specifications
- Addon version compatibility matrix
- Distribution-specific optimizations

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Development Workstation                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚     CPC     â”‚ â”‚  OpenTofu   â”‚ â”‚   Ansible   â”‚ â”‚   kubectl   â”‚â”‚
â”‚  â”‚   Control   â”‚ â”‚Infrastructureâ”‚ â”‚Configurationâ”‚ â”‚  Cluster    â”‚â”‚
â”‚  â”‚   Script    â”‚ â”‚Provisioning â”‚ â”‚ Management  â”‚ â”‚ Management  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Proxmox VE Cluster                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    Virtual Machines                        â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚â”‚
â”‚  â”‚  â”‚ Control Plane   â”‚ â”‚  Worker 1   â”‚ â”‚  Worker 2   â”‚      â”‚â”‚
â”‚  â”‚  â”‚ cu1.bevz.net    â”‚ â”‚wu1.bevz.net â”‚ â”‚wu2.bevz.net â”‚      â”‚â”‚
â”‚  â”‚  â”‚ 10.10.10.116    â”‚ â”‚10.10.10.121 â”‚ â”‚10.10.10.120 â”‚      â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Kubernetes Cluster                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  API Server â”‚ etcd â”‚ Controller â”‚ Scheduler â”‚ CNI (Calico)  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              Container Workloads & Services                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Architecture

### 1. Control Layer (CPC Script)

The **Cluster Provisioning Control** (CPC) script serves as the unified interface for all cluster operations:

```bash
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CPC Script                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Context Management    â”‚  Infrastructure      â”‚  Cluster   â”‚
â”‚  â€¢ Workspace Control  â”‚  â€¢ VM Provisioning   â”‚  â€¢ Node Mgmtâ”‚
â”‚  â€¢ Environment Vars   â”‚  â€¢ Template Creation  â”‚  â€¢ K8s Ops â”‚
â”‚  â€¢ Secret Loading     â”‚  â€¢ Network Config     â”‚  â€¢ Addons  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features:**
- **Multi-context support** (debian, ubuntu, rocky, suse)
- **Workspace isolation** per environment
- **Integrated secret management** with SOPS
- **Version-aware configurations** per OS distribution

### 2. Infrastructure Layer (Terraform/OpenTofu)

Infrastructure provisioning is handled through Infrastructure as Code principles:

```hcl
# File Structure
terraform/
â”œâ”€â”€ main.tf              # Primary resource definitions
â”œâ”€â”€ variables.tf         # Input parameters
â”œâ”€â”€ outputs.tf           # Resource outputs (IPs, FQDNs)
â”œâ”€â”€ locals.tf            # Node configuration mapping
â”œâ”€â”€ providers.tf         # Proxmox provider configuration
â”œâ”€â”€ backend.tf           # State management
â”œâ”€â”€ environments/        # Per-context variable files
â”‚   â”œâ”€â”€ ubuntu.tfvars
â”‚   â”œâ”€â”€ debian.tfvars
â”‚   â””â”€â”€ rocky.tfvars
â””â”€â”€ secrets.sops.yaml    # Encrypted credentials
```

**Resource Management:**
- **VM Templates**: Pre-configured with cloud-init
- **Network Configuration**: Static IP allocation
- **Storage Management**: System and container storage
- **Security Groups**: Firewall and access rules

### 3. Configuration Management Layer (Ansible)

Ansible handles system configuration and Kubernetes deployment:

```yaml
# Playbook Architecture
ansible/
â”œâ”€â”€ ansible.cfg          # Ansible configuration
â”œâ”€â”€ inventory/
â”‚   â””â”€â”€ tofu_inventory.py # Dynamic inventory from Terraform
â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ install_kubernetes_cluster.yml    # Component installation
â”‚   â”œâ”€â”€ initialize_kubernetes_cluster.yml # Cluster bootstrap
â”‚   â””â”€â”€ validate_cluster.yml              # Post-deployment validation
â””â”€â”€ roles/               # Reusable configuration modules
```

**Configuration Phases:**
1. **System Preparation**: OS updates, package installation
2. **Container Runtime**: containerd installation and configuration
3. **Kubernetes Components**: kubelet, kubeadm, kubectl installation
4. **Cluster Initialization**: Control plane and worker node setup
5. **Network Configuration**: CNI plugin installation (Calico)

### 4. Orchestration Layer (Kubernetes)

The Kubernetes cluster implements a standard control plane architecture:

```
Control Plane Node (cu1.bevz.net):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  kube-apiserver  â”‚  etcd  â”‚  kube-controller-manager    â”‚
â”‚  kube-scheduler  â”‚  kubelet  â”‚  kube-proxy  â”‚  calico   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Worker Nodes (wu1.bevz.net, wu2.bevz.net):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  kubelet  â”‚  kube-proxy  â”‚  calico-node  â”‚  containerd â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Network Architecture

### IP Address Allocation

```
Network Segmentation:
â”œâ”€â”€ Node Network:      10.10.10.0/24
â”‚   â”œâ”€â”€ Control Plane: 10.10.10.116
â”‚   â”œâ”€â”€ Worker 1:      10.10.10.121
â”‚   â””â”€â”€ Worker 2:      10.10.10.120
â”œâ”€â”€ Pod Network:       192.168.0.0/16
â”‚   â”œâ”€â”€ Node 1 Pods:   192.168.189.0/24
â”‚   â”œâ”€â”€ Node 2 Pods:   192.168.243.0/24
â”‚   â””â”€â”€ Node 3 Pods:   192.168.X.0/24
â””â”€â”€ Service Network:   10.96.0.0/12
    â”œâ”€â”€ Kubernetes API: 10.96.0.1
    â”œâ”€â”€ CoreDNS:        10.96.0.10
    â””â”€â”€ Services:       10.96.0.0/12
```

### Network Flow

```
External Traffic â†’ LoadBalancer â†’ Ingress â†’ Service â†’ Pod
     â†“
DNS Resolution: Pi-hole â†’ Node IPs
     â†“
Pod-to-Pod: Calico CNI â†’ BGP Routing â†’ Direct Communication
     â†“
Service Discovery: CoreDNS â†’ Service DNS â†’ Pod IPs
```

## Data Flow Architecture

### 1. Deployment Flow

```mermaid
graph TD
    A[CPC Command] --> B[Load Context & Secrets]
    B --> C[Generate Terraform Plan]
    C --> D[Apply Infrastructure]
    D --> E[Update Inventory]
    E --> F[Run Ansible Playbooks]
    F --> G[Initialize Kubernetes]
    G --> H[Install CNI]
    H --> I[Join Worker Nodes]
    I --> J[Validate Cluster]
```

### 2. Configuration Flow

```mermaid
graph LR
    A[secrets.sops.yaml] --> B[SOPS Decrypt]
    B --> C[Environment Variables]
    C --> D[Terraform Variables]
    D --> E[VM Creation]
    E --> F[Ansible Inventory]
    F --> G[Playbook Execution]
```

### 3. State Management

```
State Storage:
â”œâ”€â”€ Terraform State
â”‚   â”œâ”€â”€ Backend: S3/Local
â”‚   â”œâ”€â”€ Workspaces: per-context isolation
â”‚   â””â”€â”€ Lock Management: concurrent access prevention
â”œâ”€â”€ Ansible Facts
â”‚   â”œâ”€â”€ Dynamic Inventory: Real-time VM discovery
â”‚   â”œâ”€â”€ Host Variables: Node-specific configuration
â”‚   â””â”€â”€ Group Variables: Cluster-wide settings
â””â”€â”€ Kubernetes State
    â”œâ”€â”€ etcd: Cluster state storage
    â”œâ”€â”€ ConfigMaps: Application configuration
    â””â”€â”€ Secrets: Sensitive data management
```

## Security Architecture

### 1. Access Control

```
Authentication & Authorization:
â”œâ”€â”€ Infrastructure Level
â”‚   â”œâ”€â”€ Proxmox API: Username/Password authentication
â”‚   â”œâ”€â”€ SSH Access: Key-based authentication only
â”‚   â””â”€â”€ SOPS: PGP/KMS encryption for secrets
â”œâ”€â”€ Kubernetes Level
â”‚   â”œâ”€â”€ API Server: Certificate-based authentication
â”‚   â”œâ”€â”€ RBAC: Role-based access control
â”‚   â””â”€â”€ Service Accounts: Pod identity management
â””â”€â”€ Network Level
    â”œâ”€â”€ Firewall Rules: Port and protocol restrictions
    â”œâ”€â”€ CNI Policies: Pod-to-pod communication control
    â””â”€â”€ DNS Security: Hostname resolution control
```

### 2. Secret Management

```
Secret Lifecycle:
â”œâ”€â”€ Creation: SOPS encryption with PGP/KMS
â”œâ”€â”€ Storage: Encrypted files in version control
â”œâ”€â”€ Distribution: Runtime decryption by authorized users
â”œâ”€â”€ Usage: Environment variable injection
â””â”€â”€ Rotation: Manual key rotation process
```

### 3. Network Security

```
Network Isolation:
â”œâ”€â”€ VM Level: Proxmox firewall and network ACLs
â”œâ”€â”€ Host Level: iptables and system firewalls  
â”œâ”€â”€ Pod Level: Calico network policies
â””â”€â”€ Service Level: Kubernetes network policies
```

## Scalability Architecture

### 1. Horizontal Scaling

```
Node Scaling:
â”œâ”€â”€ Worker Nodes: Add via Terraform and join to cluster
â”œâ”€â”€ Control Plane: Multi-master setup (future enhancement)
â”œâ”€â”€ Storage: Distributed storage solutions (Longhorn/Rook)
â””â”€â”€ Network: Calico scaling for larger pod networks
```

### 2. Resource Scaling

```
Resource Management:
â”œâ”€â”€ CPU: Dynamic allocation per workload requirements
â”œâ”€â”€ Memory: Configurable per node role and workload
â”œâ”€â”€ Storage: Expandable volumes and storage classes
â””â”€â”€ Network: Bandwidth management and QoS
```

### 3. Multi-Environment Support

```
Environment Isolation:
â”œâ”€â”€ Development: Single-node or minimal cluster
â”œâ”€â”€ Staging: Production-like multi-node setup
â”œâ”€â”€ Production: High-availability with redundancy
â””â”€â”€ Testing: Ephemeral clusters for CI/CD
```

## Monitoring and Observability

### 1. Infrastructure Monitoring

```
Infrastructure Metrics:
â”œâ”€â”€ Proxmox: VM resource utilization and health
â”œâ”€â”€ Node Level: CPU, memory, disk, network metrics
â”œâ”€â”€ Container Runtime: containerd performance metrics
â””â”€â”€ Network: Calico performance and connectivity
```

### 2. Application Monitoring

```
Kubernetes Metrics:
â”œâ”€â”€ Cluster State: Node and pod health
â”œâ”€â”€ Resource Usage: CPU, memory, storage consumption
â”œâ”€â”€ Network Traffic: Inter-pod and service communication
â””â”€â”€ Application Performance: Custom metrics collection
```

### 3. Logging Architecture

```
Log Aggregation:
â”œâ”€â”€ System Logs: journald and syslog collection
â”œâ”€â”€ Container Logs: kubectl logs aggregation
â”œâ”€â”€ Application Logs: Structured logging best practices
â””â”€â”€ Audit Logs: Kubernetes API audit trail
```

## Disaster Recovery Architecture

### 1. Backup Strategy

```
Backup Components:
â”œâ”€â”€ Infrastructure: Terraform state and configurations
â”œâ”€â”€ etcd: Kubernetes cluster state backups
â”œâ”€â”€ Persistent Volumes: Application data backups
â””â”€â”€ Configuration: Ansible playbooks and secrets
```

### 2. Recovery Procedures

```
Recovery Scenarios:
â”œâ”€â”€ Node Failure: Automatic pod rescheduling
â”œâ”€â”€ Control Plane Failure: etcd restore and rebuild
â”œâ”€â”€ Complete Cluster Loss: Full rebuild from code
â””â”€â”€ Data Loss: Volume snapshots and backup restore
```

## Performance Optimization

### 1. Resource Optimization

```
Performance Tuning:
â”œâ”€â”€ CPU: Proper core allocation and affinity
â”œâ”€â”€ Memory: Optimized JVM and container limits
â”œâ”€â”€ Storage: SSD usage and I/O optimization
â””â”€â”€ Network: Bandwidth allocation and QoS
```

### 2. Application Optimization

```
Workload Optimization:
â”œâ”€â”€ Pod Scheduling: Node affinity and anti-affinity
â”œâ”€â”€ Resource Requests: Proper CPU and memory sizing
â”œâ”€â”€ Health Checks: Liveness and readiness probes
â””â”€â”€ Autoscaling: HPA and VPA configuration
```

This architecture provides a robust, scalable, and maintainable foundation for Kubernetes cluster deployment and management on Proxmox infrastructure.
